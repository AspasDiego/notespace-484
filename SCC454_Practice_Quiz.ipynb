{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Practice Quiz\n",
        "\n",
        "**Duration:** 1 Hour  \n",
        "**Total Marks:** 100  \n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. This is a **practice quiz** to help you prepare for the actual assessment.\n",
        "2. **Write your code** in the designated code cells below each question.\n",
        "3. **All questions are independent** — if you cannot answer one question, move on to the next.\n",
        "4. **Run your code** to verify correctness.\n",
        "\n",
        "### API Documentation\n",
        "\n",
        "- **NumPy:** [https://numpy.org/doc/stable/reference/](https://numpy.org/doc/stable/reference/)\n",
        "- **Pandas:** [https://pandas.pydata.org/docs/reference/](https://pandas.pydata.org/docs/reference/)\n",
        "- **Scikit-learn:** [https://scikit-learn.org/stable/api/](https://scikit-learn.org/stable/api/)\n",
        "- **PySpark SQL Functions:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/functions.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/functions.html)\n",
        "- **PySpark DataFrame:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/dataframe.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/dataframe.html)\n",
        "- **PySpark ML Feature:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.ml.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.ml.html)\n",
        "\n",
        "---\n",
        "\n",
        "| Section | Topic | Marks |\n",
        "|---------|-------|-------|\n",
        "| **A** | Python, NumPy, Pandas & Scikit-learn | **30** |\n",
        "| **B** | Apache Spark (RDDs, DataFrames, SQL) | **35** |\n",
        "| **C** | Data Preprocessing & Similarity Search | **35** |\n",
        "| | **Total** | **100** |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section A: Python, NumPy, Pandas & Scikit-learn (30 marks)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1 — NumPy Array Operations [10 marks]\n",
        "\n",
        "Consider the following 3×4 matrix **M**:\n",
        "\n",
        "```\n",
        "      Col0  Col1  Col2  Col3\n",
        "Row0    4    12     7     3\n",
        "Row1    8     5    14    10\n",
        "Row2    6    11     2     9\n",
        "```\n",
        "\n",
        "**API Reference:** [numpy.org/doc/stable/reference](https://numpy.org/doc/stable/reference/)\n",
        "\n",
        "**(a)** Create the matrix `M` as a NumPy array exactly as shown above. Print its shape and data type. **[2 marks]**\n",
        "\n",
        "**(b)** Extract and print: (i) the second row, (ii) the third column, and (iii) the element at row 1, column 2. **[2 marks]**\n",
        "\n",
        "**(c)** Compute and print the **sum** of each row and the **mean** of each column. **[3 marks]**\n",
        "\n",
        "**(d)** Using boolean indexing, find and print all elements in `M` that are **greater than 7**. Then, create a copy of `M` and replace all elements greater than 7 with `0`. Print the modified matrix. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 4)\n",
            "int64\n",
            "[ 8  5 14 10]\n",
            "[ 7 14  2]\n",
            "14\n",
            "Sum of each row = [26 37 28]\n",
            "Mean of each column = [6.         9.33333333 7.66666667 7.33333333]\n",
            "Elements > 7 = [12  8 14 10 11  9]\n",
            "Updated Matrix:\n",
            "[[4 0 7 3]\n",
            " [0 5 0 0]\n",
            " [6 0 2 0]]\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your code here\n",
        "import numpy as np\n",
        "\n",
        "# (a) Create matrix M, print shape and dtype\n",
        "nums = [4, 12, 7, 3, 8, 5, 14, 10, 6, 11, 2, 9]\n",
        "\n",
        "M = np.array(nums).reshape(3, 4)\n",
        "\n",
        "print(M.shape)\n",
        "print(M.dtype)\n",
        "\n",
        "# (b) Extract second row, third column, element at [1,2]\n",
        "print(M[1, :])\n",
        "print(M[:, 2])\n",
        "print(M[1, 2])\n",
        "\n",
        "# (c) Sum of each row, mean of each column\n",
        "row_sum = M.sum(axis=1)\n",
        "col_mean = M.mean(axis=0)\n",
        "\n",
        "print(\"Sum of each row =\", row_sum)\n",
        "print(\"Mean of each column =\", col_mean)\n",
        "\n",
        "# (d) Elements > 7, then replace > 7 with 0\n",
        "print(\"Elements > 7 =\", M[M > 7])\n",
        "\n",
        "M[M > 7] = 0\n",
        "\n",
        "print(\"Updated Matrix:\")\n",
        "print(M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2 — Pandas Data Manipulation [10 marks]\n",
        "\n",
        "A shop has recorded the following sales data:\n",
        "\n",
        "```\n",
        "order_id  product      category     price   quantity  date\n",
        "1001      Laptop       Electronics  999.99  1         2025-03-01\n",
        "1002      Mouse        Electronics  29.99   3         2025-03-01\n",
        "1003      Notebook     Stationery   5.99    10        2025-03-02\n",
        "1004      Keyboard     Electronics  79.99   2         2025-03-02\n",
        "1005      Pen Set      Stationery   12.99   5         2025-03-03\n",
        "1006      Monitor      Electronics  349.99  1         2025-03-03\n",
        "1007      Stapler      Stationery   8.99    NaN       2025-03-04\n",
        "1008      Headphones   Electronics  149.99  2         2025-03-04\n",
        "```\n",
        "\n",
        "**API Reference:** [pandas.pydata.org/docs/reference](https://pandas.pydata.org/docs/reference/)\n",
        "\n",
        "**(a)** Create this DataFrame in pandas exactly as shown (use `np.nan` for the missing value). Print the DataFrame and its info. **[2 marks]**\n",
        "\n",
        "**(b)** Fill the missing `quantity` value with the **median** quantity of all products. Print the updated DataFrame. **[2 marks]**\n",
        "\n",
        "**(c)** Add a new column called `total` computed as `price × quantity`. Then filter and display only rows where `total > 100`. **[3 marks]**\n",
        "\n",
        "**(d)** Using `groupby`, calculate the **total revenue** (sum of `total`) and the **number of orders** per category. Sort by total revenue descending. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id     product     category   price  quantity       date\n",
            "0      1001      Laptop  Electronics  999.99       1.0 2025-03-01\n",
            "1      1002       Mouse  Electronics   29.99       3.0 2025-03-01\n",
            "2      1003    Notebook   Stationery    5.99      10.0 2025-03-02\n",
            "3      1004    Keyboard  Electronics   79.99       2.0 2025-03-02\n",
            "4      1005     Pen Set   Stationery   12.99       5.0 2025-03-03\n",
            "5      1006     Monitor  Electronics  349.99       1.0 2025-03-03\n",
            "6      1007     Stapler   Stationery    8.99       NaN 2025-03-04\n",
            "7      1008  Headphones  Electronics  149.99       2.0 2025-03-04\n",
            "<class 'pandas.DataFrame'>\n",
            "RangeIndex: 8 entries, 0 to 7\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count  Dtype         \n",
            "---  ------    --------------  -----         \n",
            " 0   order_id  8 non-null      int64         \n",
            " 1   product   8 non-null      str           \n",
            " 2   category  8 non-null      str           \n",
            " 3   price     8 non-null      float64       \n",
            " 4   quantity  7 non-null      float64       \n",
            " 5   date      8 non-null      datetime64[us]\n",
            "dtypes: datetime64[us](1), float64(2), int64(1), str(2)\n",
            "memory usage: 516.0 bytes\n",
            "None\n",
            "\n",
            "After filling missing quantity:\n",
            "   order_id     product     category   price  quantity       date\n",
            "0      1001      Laptop  Electronics  999.99       1.0 2025-03-01\n",
            "1      1002       Mouse  Electronics   29.99       3.0 2025-03-01\n",
            "2      1003    Notebook   Stationery    5.99      10.0 2025-03-02\n",
            "3      1004    Keyboard  Electronics   79.99       2.0 2025-03-02\n",
            "4      1005     Pen Set   Stationery   12.99       5.0 2025-03-03\n",
            "5      1006     Monitor  Electronics  349.99       1.0 2025-03-03\n",
            "6      1007     Stapler   Stationery    8.99       2.0 2025-03-04\n",
            "7      1008  Headphones  Electronics  149.99       2.0 2025-03-04\n",
            "\n",
            "Rows where total > 100:\n",
            "   order_id     product     category   price  quantity       date   total\n",
            "0      1001      Laptop  Electronics  999.99       1.0 2025-03-01  999.99\n",
            "3      1004    Keyboard  Electronics   79.99       2.0 2025-03-02  159.98\n",
            "5      1006     Monitor  Electronics  349.99       1.0 2025-03-03  349.99\n",
            "7      1008  Headphones  Electronics  149.99       2.0 2025-03-04  299.98\n",
            "\n",
            "Revenue summary by category:\n",
            "             total_revenue  number_of_orders\n",
            "category                                    \n",
            "Electronics        1899.91                 5\n",
            "Stationery          142.83                 3\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your code here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# (a) Create DataFrame and print info\n",
        "data = {\n",
        "    \"order_id\": [1001,1002,1003,1004,1005,1006,1007,1008],\n",
        "    \"product\": [\"Laptop\",\"Mouse\",\"Notebook\",\"Keyboard\",\"Pen Set\",\"Monitor\",\"Stapler\",\"Headphones\"],\n",
        "    \"category\": [\"Electronics\",\"Electronics\",\"Stationery\",\"Electronics\",\"Stationery\",\"Electronics\",\"Stationery\",\"Electronics\"],\n",
        "    \"price\": [999.99,29.99,5.99,79.99,12.99,349.99,8.99,149.99],\n",
        "    \"quantity\": [1,3,10,2,5,1,np.nan,2],\n",
        "    \"date\": pd.to_datetime([\n",
        "        \"2025-03-01\",\"2025-03-01\",\"2025-03-02\",\"2025-03-02\",\n",
        "        \"2025-03-03\",\"2025-03-03\",\"2025-03-04\",\"2025-03-04\"\n",
        "    ])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)\n",
        "print(df.info())\n",
        "\n",
        "# (b) Fill missing quantity with median\n",
        "median_qty = df[\"quantity\"].median()\n",
        "df[\"quantity\"] = df[\"quantity\"].fillna(median_qty)\n",
        "\n",
        "print(\"\\nAfter filling missing quantity:\")\n",
        "print(df)\n",
        "\n",
        "# (c) Add total column, filter where total > 100\n",
        "df[\"total\"] = df[\"price\"] * df[\"quantity\"]\n",
        "\n",
        "filtered = df[df[\"total\"] > 100]\n",
        "\n",
        "print(\"\\nRows where total > 100:\")\n",
        "print(filtered)\n",
        "\n",
        "# (d) Groupby category: total revenue and order count\n",
        "summary = (\n",
        "    df.groupby(\"category\")\n",
        "      .agg(\n",
        "          total_revenue=(\"total\", \"sum\"),\n",
        "          number_of_orders=(\"order_id\", \"count\")\n",
        "      )\n",
        "      .sort_values(by=\"total_revenue\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nRevenue summary by category:\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 — Scikit-learn Classification [10 marks]\n",
        "\n",
        "You will use the Iris dataset for this question. **Run the setup cell first.**\n",
        "\n",
        "**API Reference:** [scikit-learn.org/stable/api](https://scikit-learn.org/stable/api/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (150, 5)\n",
            "Target classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
            "Class distribution:\n",
            "target\n",
            "0    50\n",
            "1    50\n",
            "2    50\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target  \n",
              "0       0  \n",
              "1       0  \n",
              "2       0  \n",
              "3       0  \n",
              "4       0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['target'] = iris.target\n",
        "\n",
        "print(f\"Dataset shape: {df_iris.shape}\")\n",
        "print(f\"Target classes: {list(iris.target_names)}\")\n",
        "print(f\"Class distribution:\\n{df_iris['target'].value_counts().sort_index()}\")\n",
        "df_iris.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Split the data into training (80%) and testing (20%) sets with `random_state=42` and stratified sampling. Print the shapes. **[2 marks]**\n",
        "\n",
        "**(b)** Apply `StandardScaler` to the features. Fit on training data only, then transform both sets. **[2 marks]**\n",
        "\n",
        "**(c)** Train a **K-Nearest Neighbours** classifier with `n_neighbors=3`. Print the accuracy on the test set. **[3 marks]**\n",
        "\n",
        "**(d)** Print the **confusion matrix** and the **classification report** for the KNN model. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (120, 4)\n",
            "X_test shape: (30, 4)\n",
            "y_train shape: (120,)\n",
            "y_test shape: (30,)\n",
            "Test Accuracy: 0.9333333333333333\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  2  8]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.83      1.00      0.91        10\n",
            "           2       1.00      0.80      0.89        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.93      0.93        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q3 — Write your code here\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# (a) Train-test split with stratification\n",
        "X = df_iris.drop(\"target\", axis=1)\n",
        "y = df_iris[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# (b) StandardScaler - fit on train, transform both\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# (c) Train KNN with n_neighbors=3, print accuracy\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# (d) Confusion matrix and classification report\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section B: Apache Spark — RDDs, DataFrames & SQL (35 marks)\n",
        "---\n",
        "\n",
        "### ⚙️ Spark Setup\n",
        "\n",
        "Run the two setup cells below before attempting the Spark questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PySpark and Java installed successfully!\n",
            "Driver Python: d:\\Lancaster University Coursework\\Term 2\\SSC 454 - Large scale platforms for AI and Data Analysis\\Labs\\venv\\Scripts\\python.exe\n",
            "Worker Python: d:\\Lancaster University Coursework\\Term 2\\SSC 454 - Large scale platforms for AI and Data Analysis\\Labs\\venv\\Scripts\\python.exe\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        }
      ],
      "source": [
        "# === SETUP CELL 1: Install PySpark and Java ===\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null 2>&1\n",
        "\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-17\"\n",
        "print(\"PySpark and Java installed successfully!\")\n",
        "\n",
        "# Remove before submit\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
        "\n",
        "print(\"Driver Python:\", sys.executable)\n",
        "print(\"Worker Python:\", os.environ[\"PYSPARK_PYTHON\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 3.5.0\n"
          ]
        }
      ],
      "source": [
        "# === SETUP CELL 2: Create SparkSession ===\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-Practice\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4 — RDD Transformations and Actions [12 marks]\n",
        "\n",
        "**Run the setup cell first**, then answer the questions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RDD created with 5 sentences\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "sentences = [\n",
        "    \"Apache Spark is fast\",\n",
        "    \"Spark is used for big data\",\n",
        "    \"Big data processing is important\",\n",
        "    \"Spark and Hadoop are popular\",\n",
        "    \"Data science uses Spark\",\n",
        "]\n",
        "\n",
        "sentences_rdd = sc.parallelize(sentences, 2)\n",
        "print(f\"RDD created with {sentences_rdd.count()} sentences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Using `flatMap`, split each sentence into words (lowercase) and collect all words as a list. Print the total number of words. **[3 marks]**\n",
        "\n",
        "**(b)** Using `map` and `reduceByKey`, count the occurrences of each word. Print all word counts. **[3 marks]**\n",
        "\n",
        "**(c)** Find the **top 5 most frequent words** using `sortBy`. Print them with their counts. **[3 marks]**\n",
        "\n",
        "**(d)** Using `filter`, find all words that contain the letter `'a'`. Print the count and the list of words. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words: 24\n",
            "\n",
            "Word Counts:\n",
            "apache 1\n",
            "fast 1\n",
            "used 1\n",
            "for 1\n",
            "big 2\n",
            "important 1\n",
            "and 1\n",
            "hadoop 1\n",
            "are 1\n",
            "popular 1\n",
            "science 1\n",
            "uses 1\n",
            "spark 4\n",
            "is 3\n",
            "data 3\n",
            "processing 1\n",
            "\n",
            "Top 5 Most Frequent Words:\n",
            "spark 4\n",
            "is 3\n",
            "data 3\n",
            "big 2\n",
            "apache 1\n",
            "\n",
            "Words containing 'a':\n",
            "Count: 14\n",
            "Words: ['apache', 'spark', 'fast', 'spark', 'data', 'data', 'important', 'spark', 'and', 'hadoop', 'are', 'popular', 'data', 'spark']\n"
          ]
        }
      ],
      "source": [
        "# Q4 — Write your code here\n",
        "\n",
        "# (a) Split sentences into words, count total words\n",
        "words_rdd = sentences_rdd.flatMap(lambda x: x.lower().split())\n",
        "\n",
        "total_words = words_rdd.count()\n",
        "print(\"Total number of words:\", total_words)\n",
        "\n",
        "# (b) Word count using map and reduceByKey\n",
        "word_counts = (\n",
        "    words_rdd\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "print(\"\\nWord Counts:\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(word, count)\n",
        "\n",
        "# (c) Top 5 most frequent words\n",
        "top5 = (\n",
        "    word_counts\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        "    .take(5)\n",
        ")\n",
        "\n",
        "print(\"\\nTop 5 Most Frequent Words:\")\n",
        "for word, count in top5:\n",
        "    print(word, count)\n",
        "\n",
        "# (d) Words containing letter 'a'\n",
        "words_with_a = words_rdd.filter(lambda word: 'a' in word)\n",
        "\n",
        "filtered_words = words_with_a.collect()\n",
        "\n",
        "print(\"\\nWords containing 'a':\")\n",
        "print(\"Count:\", len(filtered_words))\n",
        "print(\"Words:\", filtered_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5 — Spark DataFrame Operations [12 marks]\n",
        "\n",
        "Consider the following student grades data:\n",
        "\n",
        "```\n",
        "student_id  name      subject     score   semester\n",
        "S001        Alice     Maths       85      Fall\n",
        "S001        Alice     Physics     78      Fall\n",
        "S002        Bob       Maths       92      Fall\n",
        "S002        Bob       Physics     88      Fall\n",
        "S003        Carol     Maths       76      Fall\n",
        "S003        Carol     Physics     82      Fall\n",
        "S001        Alice     Maths       88      Spring\n",
        "S001        Alice     Physics     84      Spring\n",
        "S002        Bob       Maths       90      Spring\n",
        "S002        Bob       Physics     91      Spring\n",
        "```\n",
        "\n",
        "**Run the setup cell first.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grades DataFrame created:\n",
            "+----------+-----+-------+-----+--------+\n",
            "|student_id| name|subject|score|semester|\n",
            "+----------+-----+-------+-----+--------+\n",
            "|      S001|Alice|  Maths|   85|    Fall|\n",
            "|      S001|Alice|Physics|   78|    Fall|\n",
            "|      S002|  Bob|  Maths|   92|    Fall|\n",
            "|      S002|  Bob|Physics|   88|    Fall|\n",
            "|      S003|Carol|  Maths|   76|    Fall|\n",
            "|      S003|Carol|Physics|   82|    Fall|\n",
            "|      S001|Alice|  Maths|   88|  Spring|\n",
            "|      S001|Alice|Physics|   84|  Spring|\n",
            "|      S002|  Bob|  Maths|   90|  Spring|\n",
            "|      S002|  Bob|Physics|   91|  Spring|\n",
            "+----------+-----+-------+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "grades_data = [\n",
        "    (\"S001\", \"Alice\", \"Maths\", 85, \"Fall\"),\n",
        "    (\"S001\", \"Alice\", \"Physics\", 78, \"Fall\"),\n",
        "    (\"S002\", \"Bob\", \"Maths\", 92, \"Fall\"),\n",
        "    (\"S002\", \"Bob\", \"Physics\", 88, \"Fall\"),\n",
        "    (\"S003\", \"Carol\", \"Maths\", 76, \"Fall\"),\n",
        "    (\"S003\", \"Carol\", \"Physics\", 82, \"Fall\"),\n",
        "    (\"S001\", \"Alice\", \"Maths\", 88, \"Spring\"),\n",
        "    (\"S001\", \"Alice\", \"Physics\", 84, \"Spring\"),\n",
        "    (\"S002\", \"Bob\", \"Maths\", 90, \"Spring\"),\n",
        "    (\"S002\", \"Bob\", \"Physics\", 91, \"Spring\"),\n",
        "]\n",
        "\n",
        "grades_schema = StructType([\n",
        "    StructField(\"student_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"subject\", StringType(), True),\n",
        "    StructField(\"score\", IntegerType(), True),\n",
        "    StructField(\"semester\", StringType(), True),\n",
        "])\n",
        "\n",
        "grades_df = spark.createDataFrame(grades_data, grades_schema)\n",
        "print(\"Grades DataFrame created:\")\n",
        "grades_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Select only `name`, `subject`, and `score` columns. Then filter to show only rows where `score >= 85`. **[3 marks]**\n",
        "\n",
        "**(b)** Add a new column `grade` based on score: `'A'` if score >= 90, `'B'` if score >= 80, `'C'` otherwise. Show the result. **[3 marks]**\n",
        "\n",
        "**(c)** Using `groupBy`, calculate the **average score** per student (by `name`). Order by average score descending. **[3 marks]**\n",
        "\n",
        "**(d)** Using `groupBy`, calculate the **average score** per subject per semester. Show the result ordered by semester then subject. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores >= 85:\n",
            "+-----+-------+-----+\n",
            "| name|subject|score|\n",
            "+-----+-------+-----+\n",
            "|Alice|  Maths|   85|\n",
            "|  Bob|  Maths|   92|\n",
            "|  Bob|Physics|   88|\n",
            "|Alice|  Maths|   88|\n",
            "|  Bob|  Maths|   90|\n",
            "|  Bob|Physics|   91|\n",
            "+-----+-------+-----+\n",
            "\n",
            "With Grade Column:\n",
            "+----------+-----+-------+-----+--------+-----+\n",
            "|student_id| name|subject|score|semester|grade|\n",
            "+----------+-----+-------+-----+--------+-----+\n",
            "|      S001|Alice|  Maths|   85|    Fall|    B|\n",
            "|      S001|Alice|Physics|   78|    Fall|    C|\n",
            "|      S002|  Bob|  Maths|   92|    Fall|    A|\n",
            "|      S002|  Bob|Physics|   88|    Fall|    B|\n",
            "|      S003|Carol|  Maths|   76|    Fall|    C|\n",
            "|      S003|Carol|Physics|   82|    Fall|    B|\n",
            "|      S001|Alice|  Maths|   88|  Spring|    B|\n",
            "|      S001|Alice|Physics|   84|  Spring|    B|\n",
            "|      S002|  Bob|  Maths|   90|  Spring|    A|\n",
            "|      S002|  Bob|Physics|   91|  Spring|    A|\n",
            "+----------+-----+-------+-----+--------+-----+\n",
            "\n",
            "Average Score per Student:\n",
            "+-----+---------+\n",
            "| name|avg_score|\n",
            "+-----+---------+\n",
            "|  Bob|    90.25|\n",
            "|Alice|    83.75|\n",
            "|Carol|     79.0|\n",
            "+-----+---------+\n",
            "\n",
            "Average Score per Subject per Semester:\n",
            "+--------+-------+---------+\n",
            "|semester|subject|avg_score|\n",
            "+--------+-------+---------+\n",
            "|    Fall|  Maths|    84.33|\n",
            "|    Fall|Physics|    82.67|\n",
            "|  Spring|  Maths|     89.0|\n",
            "|  Spring|Physics|     87.5|\n",
            "+--------+-------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q5 — Write your code here\n",
        "from pyspark.sql.functions import col, when, avg, round as spark_round\n",
        "\n",
        "# (a) Select columns and filter score >= 85\n",
        "filtered_df = (\n",
        "    grades_df\n",
        "    .select(\"name\", \"subject\", \"score\")\n",
        "    .filter(col(\"score\") >= 85)\n",
        ")\n",
        "\n",
        "print(\"Scores >= 85:\")\n",
        "filtered_df.show()\n",
        "\n",
        "# (b) Add grade column (A/B/C based on score)\n",
        "graded_df = (\n",
        "    grades_df\n",
        "    .withColumn(\n",
        "        \"grade\",\n",
        "        when(col(\"score\") >= 90, \"A\")\n",
        "        .when(col(\"score\") >= 80, \"B\")\n",
        "        .otherwise(\"C\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"With Grade Column:\")\n",
        "graded_df.show()\n",
        "\n",
        "# (c) Average score per student\n",
        "avg_per_student = (\n",
        "    grades_df\n",
        "    .groupBy(\"name\")\n",
        "    .agg(spark_round(avg(\"score\"), 2).alias(\"avg_score\"))\n",
        "    .orderBy(col(\"avg_score\").desc())\n",
        ")\n",
        "\n",
        "print(\"Average Score per Student:\")\n",
        "avg_per_student.show()\n",
        "\n",
        "# (d) Average score per subject per semester\n",
        "avg_subject_semester = (\n",
        "    grades_df\n",
        "    .groupBy(\"semester\", \"subject\")\n",
        "    .agg(spark_round(avg(\"score\"), 2).alias(\"avg_score\"))\n",
        "    .orderBy(\"semester\", \"subject\")\n",
        ")\n",
        "\n",
        "print(\"Average Score per Subject per Semester:\")\n",
        "avg_subject_semester.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6 — Spark SQL [11 marks]\n",
        "\n",
        "Register the grades DataFrame as a temporary view and answer using **Spark SQL**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View 'grades' registered.\n"
          ]
        }
      ],
      "source": [
        "# Register the DataFrame as a temp view\n",
        "grades_df.createOrReplaceTempView(\"grades\")\n",
        "print(\"View 'grades' registered.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Write a SQL query to find all students who scored **above 85** in **Maths**. Return: `name`, `score`, `semester`. **[3 marks]**\n",
        "\n",
        "**(b)** Write a SQL query to calculate the **average score per subject**. Return: `subject`, `avg_score` (rounded to 2 decimals). **[3 marks]**\n",
        "\n",
        "**(c)** Write a SQL query to find the **highest score** achieved by each student across all subjects and semesters. Return: `name`, `max_score`. Order by `max_score` descending. **[5 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+--------+\n",
            "| name|score|semester|\n",
            "+-----+-----+--------+\n",
            "|  Bob|   92|    Fall|\n",
            "|Alice|   88|  Spring|\n",
            "|  Bob|   90|  Spring|\n",
            "+-----+-----+--------+\n",
            "\n",
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|  Maths|     86.2|\n",
            "|Physics|     84.6|\n",
            "+-------+---------+\n",
            "\n",
            "+-----+---------+\n",
            "| name|max_score|\n",
            "+-----+---------+\n",
            "|  Bob|       92|\n",
            "|Alice|       88|\n",
            "|Carol|       82|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q6 — Write your SQL queries here\n",
        "\n",
        "# (a) Students scoring above 85 in Maths\n",
        "result_a = spark.sql(\"\"\"\n",
        "    SELECT name, score, semester\n",
        "    FROM grades\n",
        "    WHERE subject = 'Maths' AND score > 85\n",
        "\"\"\")\n",
        "result_a.show()\n",
        "\n",
        "\n",
        "# (b) Average score per subject\n",
        "result_b = spark.sql(\"\"\"\n",
        "    SELECT subject,\n",
        "           ROUND(AVG(score), 2) AS avg_score\n",
        "    FROM grades\n",
        "    GROUP BY subject\n",
        "\"\"\")\n",
        "result_b.show()\n",
        "\n",
        "\n",
        "# (c) Highest score per student\n",
        "result_c = spark.sql(\"\"\"\n",
        "    SELECT name,\n",
        "           MAX(score) AS max_score\n",
        "    FROM grades\n",
        "    GROUP BY name\n",
        "    ORDER BY max_score DESC\n",
        "\"\"\")\n",
        "result_c.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section C: Data Preprocessing & Similarity Search (35 marks)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7 — Text Preprocessing & Regular Expressions [12 marks]\n",
        "\n",
        "Consider the following product data with messy text:\n",
        "\n",
        "```\n",
        "id   raw_text\n",
        "1    \"Product: LAPTOP-2025 | Price: $999.99 | Stock: 50\"\n",
        "2    \"Product: mouse-2024 | Price: $29.50 | Stock: 200\"\n",
        "3    \"Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75\"\n",
        "4    \"Product: Monitor-2023 | Price: $349.99 | Stock: 30\"\n",
        "5    \"Product: HEADSET-2025 | Price: $149.00 | Stock: 100\"\n",
        "```\n",
        "\n",
        "**Run the setup cell first.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Products DataFrame created:\n",
            "+---+---------------------------------------------------+\n",
            "|id |raw_text                                           |\n",
            "+---+---------------------------------------------------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |\n",
            "|2  |Product: mouse-2024 | Price: $29.50 | Stock: 200   |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |\n",
            "|4  |Product: Monitor-2023 | Price: $349.99 | Stock: 30 |\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|\n",
            "+---+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "product_data = [\n",
        "    (1, \"Product: LAPTOP-2025 | Price: $999.99 | Stock: 50\"),\n",
        "    (2, \"Product: mouse-2024 | Price: $29.50 | Stock: 200\"),\n",
        "    (3, \"Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75\"),\n",
        "    (4, \"Product: Monitor-2023 | Price: $349.99 | Stock: 30\"),\n",
        "    (5, \"Product: HEADSET-2025 | Price: $149.00 | Stock: 100\"),\n",
        "]\n",
        "\n",
        "products_df = spark.createDataFrame(product_data, [\"id\", \"raw_text\"])\n",
        "print(\"Products DataFrame created:\")\n",
        "products_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Using `regexp_extract`, extract the **product name** (e.g., \"LAPTOP-2025\") into a new column called `product_name`. Show the result. **[3 marks]**\n",
        "\n",
        "**(b)** Using `regexp_extract`, extract the **price** (the numeric value after $, e.g., \"999.99\") into a column called `price`. Cast it to `DoubleType`. **[3 marks]**\n",
        "\n",
        "**(c)** Using `lower()`, convert the `product_name` to lowercase. Then use `regexp_replace` to remove the year part (e.g., \"-2025\") from the product name. **[3 marks]**\n",
        "\n",
        "**(d)** Using `rlike`, filter to show only products from year **2025** (i.e., product name contains \"2025\"). **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Product Name:\n",
            "+---+---------------------------------------------------+-------------+\n",
            "|id |raw_text                                           |product_name |\n",
            "+---+---------------------------------------------------+-------------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |LAPTOP-2025  |\n",
            "|2  |Product: mouse-2024 | Price: $29.50 | Stock: 200   |mouse-2024   |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |KEYBOARD-2025|\n",
            "|4  |Product: Monitor-2023 | Price: $349.99 | Stock: 30 |Monitor-2023 |\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|HEADSET-2025 |\n",
            "+---+---------------------------------------------------+-------------+\n",
            "\n",
            "Extracted Price:\n",
            "+---+---------------------------------------------------+-------------+------+\n",
            "|id |raw_text                                           |product_name |price |\n",
            "+---+---------------------------------------------------+-------------+------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |LAPTOP-2025  |999.99|\n",
            "|2  |Product: mouse-2024 | Price: $29.50 | Stock: 200   |mouse-2024   |29.5  |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |KEYBOARD-2025|79.0  |\n",
            "|4  |Product: Monitor-2023 | Price: $349.99 | Stock: 30 |Monitor-2023 |349.99|\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|HEADSET-2025 |149.0 |\n",
            "+---+---------------------------------------------------+-------------+------+\n",
            "\n",
            "Cleaned Product Name:\n",
            "+---+---------------------------------------------------+------------+------+\n",
            "|id |raw_text                                           |product_name|price |\n",
            "+---+---------------------------------------------------+------------+------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |laptop      |999.99|\n",
            "|2  |Product: mouse-2024 | Price: $29.50 | Stock: 200   |mouse       |29.5  |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |keyboard    |79.0  |\n",
            "|4  |Product: Monitor-2023 | Price: $349.99 | Stock: 30 |monitor     |349.99|\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|headset     |149.0 |\n",
            "+---+---------------------------------------------------+------------+------+\n",
            "\n",
            "Products from 2025:\n",
            "+---+---------------------------------------------------+\n",
            "|id |raw_text                                           |\n",
            "+---+---------------------------------------------------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|\n",
            "+---+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q7 — Write your code here\n",
        "from pyspark.sql.functions import regexp_extract, regexp_replace, lower, col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# (a) Extract product name\n",
        "products_with_name = products_df.withColumn(\n",
        "    \"product_name\",\n",
        "    regexp_extract(col(\"raw_text\"), r\"Product:\\s*([A-Za-z]+-\\d{4})\", 1)\n",
        ")\n",
        "\n",
        "print(\"Extracted Product Name:\")\n",
        "products_with_name.show(truncate=False)\n",
        "\n",
        "# (b) Extract price and cast to Double\n",
        "products_with_price = products_with_name.withColumn(\n",
        "    \"price\",\n",
        "    regexp_extract(col(\"raw_text\"), r\"Price:\\s*\\$(\\d+\\.\\d+)\", 1).cast(DoubleType())\n",
        ")\n",
        "\n",
        "print(\"Extracted Price:\")\n",
        "products_with_price.show(truncate=False)\n",
        "\n",
        "# (c) Lowercase product name and remove year\n",
        "cleaned_products = products_with_price.withColumn(\n",
        "    \"product_name\",\n",
        "    lower(col(\"product_name\"))\n",
        ").withColumn(\n",
        "    \"product_name\",\n",
        "    regexp_replace(col(\"product_name\"), r\"-\\d{4}\", \"\")\n",
        ")\n",
        "\n",
        "print(\"Cleaned Product Name:\")\n",
        "cleaned_products.show(truncate=False)\n",
        "\n",
        "# (d) Filter products from 2025\n",
        "products_2025 = products_df.filter(\n",
        "    col(\"raw_text\").rlike(r\"2025\")\n",
        ")\n",
        "\n",
        "print(\"Products from 2025:\")\n",
        "products_2025.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8 — Shingling & Jaccard Similarity [12 marks]\n",
        "\n",
        "Consider these three short documents:\n",
        "\n",
        "```\n",
        "Doc A: \"the cat sat on the mat\"\n",
        "Doc B: \"the cat sat on the hat\"\n",
        "Doc C: \"the dog ran in the park\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Write a Python function `word_shingles(text, n)` that returns a **set** of word n-grams. Apply it to all three documents with `n=2`. Print the shingle sets for each document. **[3 marks]**\n",
        "\n",
        "**(b)** Write a function `jaccard_similarity(set_a, set_b)` that computes Jaccard similarity. Calculate and print the similarity between: (A, B), (A, C), and (B, C). **[3 marks]**\n",
        "\n",
        "**(c)** Based on your results, which pair of documents is **most similar**? Which is **least similar**? **[2 marks]**\n",
        "\n",
        "**(d)** Write a simple `MinHash` function that takes a set and `num_hashes` parameter, and returns a signature (list of minimum hash values). Use Python's built-in `hash()` function with different salts. Compare the estimated Jaccard (from signatures) with the true Jaccard for documents A and B using `num_hashes=50`. **[4 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shingles A: {('on', 'the'), ('sat', 'on'), ('the', 'cat'), ('cat', 'sat'), ('the', 'mat')}\n",
            "Shingles B: {('on', 'the'), ('sat', 'on'), ('the', 'cat'), ('cat', 'sat'), ('the', 'hat')}\n",
            "Shingles C: {('the', 'dog'), ('ran', 'in'), ('in', 'the'), ('the', 'park'), ('dog', 'ran')}\n",
            "\n",
            "Jaccard Similarities:\n",
            "A vs B: 0.6666666666666666\n",
            "A vs C: 0.0\n",
            "B vs C: 0.0\n",
            "\n",
            "Most similar pair: A & B\n",
            "Least similar pair: A & C (or B & C)\n",
            "\n",
            "True Jaccard (A vs B): 0.6666666666666666\n",
            "Estimated Jaccard (A vs B) using MinHash: 0.62\n"
          ]
        }
      ],
      "source": [
        "# Q8 — Write your code here\n",
        "import builtins\n",
        "\n",
        "# Documents\n",
        "doc_a = \"the cat sat on the mat\"\n",
        "doc_b = \"the cat sat on the hat\"\n",
        "doc_c = \"the dog ran in the park\"\n",
        "\n",
        "\n",
        "# (a) Word shingles function, apply with n=2\n",
        "def word_shingles(text, n):\n",
        "    words = text.split()\n",
        "    return set(tuple(words[i:i+n]) for i in range(len(words) - n + 1))\n",
        "\n",
        "shingles_a = word_shingles(doc_a, 2)\n",
        "shingles_b = word_shingles(doc_b, 2)\n",
        "shingles_c = word_shingles(doc_c, 2)\n",
        "\n",
        "print(\"Shingles A:\", shingles_a)\n",
        "print(\"Shingles B:\", shingles_b)\n",
        "print(\"Shingles C:\", shingles_c)\n",
        "\n",
        "\n",
        "# (b) Jaccard similarity function and compute for all pairs\n",
        "def jaccard_similarity(set_a, set_b):\n",
        "    return len(set_a & set_b) / len(set_a | set_b)\n",
        "\n",
        "sim_ab = jaccard_similarity(shingles_a, shingles_b)\n",
        "sim_ac = jaccard_similarity(shingles_a, shingles_c)\n",
        "sim_bc = jaccard_similarity(shingles_b, shingles_c)\n",
        "\n",
        "print(\"\\nJaccard Similarities:\")\n",
        "print(\"A vs B:\", sim_ab)\n",
        "print(\"A vs C:\", sim_ac)\n",
        "print(\"B vs C:\", sim_bc)\n",
        "\n",
        "\n",
        "# (c) Most similar and least similar pairs\n",
        "print(\"\\nMost similar pair: A & B\")\n",
        "print(\"Least similar pair: A & C (or B & C)\")\n",
        "\n",
        "\n",
        "# (d) Simple MinHash function and comparison\n",
        "def minhash_signature(shingle_set, num_hashes):\n",
        "    signature = []\n",
        "    for i in range(num_hashes):\n",
        "        min_hash = min(hash(str(shingle) + str(i)) for shingle in shingle_set)\n",
        "        signature.append(min_hash)\n",
        "    return signature\n",
        "\n",
        "def estimated_jaccard(sig1, sig2):\n",
        "    matches = builtins.sum(1 for i in range(len(sig1)) if sig1[i] == sig2[i])\n",
        "    return matches / len(sig1)\n",
        "\n",
        "num_hashes = 50\n",
        "\n",
        "sig_a = minhash_signature(shingles_a, num_hashes)\n",
        "sig_b = minhash_signature(shingles_b, num_hashes)\n",
        "\n",
        "est_sim_ab = estimated_jaccard(sig_a, sig_b)\n",
        "\n",
        "print(\"\\nTrue Jaccard (A vs B):\", sim_ab)\n",
        "print(\"Estimated Jaccard (A vs B) using MinHash:\", est_sim_ab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 9 — LSH with Spark ML [11 marks]\n",
        "\n",
        "Using the same three documents from Question 8, build an LSH pipeline with Spark ML.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Create a Spark DataFrame with columns `id` and `text` for the three documents. Use `Tokenizer` to split into words, then `CountVectorizer` with `binary=True` to create feature vectors. Show the schema. **[3 marks]**\n",
        "\n",
        "**(b)** Fit a `MinHashLSH` model with `numHashTables=3`. Transform the data and show the hash values. **[3 marks]**\n",
        "\n",
        "**(c)** Use `approxSimilarityJoin` with threshold `0.6` to find similar document pairs. Display the results. **[3 marks]**\n",
        "\n",
        "**(d)** Use `approxNearestNeighbors` to find the 2 nearest neighbours of document A. Print their IDs and distances. **[2 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema after vectorization:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "Hashed Data:\n",
            "+---+-----------------------------------------------+\n",
            "|id |hashes                                         |\n",
            "+---+-----------------------------------------------+\n",
            "|1  |[[1.4369549E7], [7.35564722E8], [3.14786228E8]]|\n",
            "|2  |[[1.4369549E7], [7.35564722E8], [3.14786228E8]]|\n",
            "|3  |[[1.4369549E7], [7.35564722E8], [3.11263053E8]]|\n",
            "+---+-----------------------------------------------+\n",
            "\n",
            "Similar document pairs (threshold=0.6):\n",
            "+----+----+-------------------+\n",
            "|doc1|doc2|    JaccardDistance|\n",
            "+----+----+-------------------+\n",
            "|   1|   2|0.33333333333333337|\n",
            "+----+----+-------------------+\n",
            "\n",
            "2 Nearest Neighbours of Document A:\n",
            "+---+-------------------+\n",
            "| id|    JaccardDistance|\n",
            "+---+-------------------+\n",
            "|  1|                0.0|\n",
            "|  2|0.33333333333333337|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q9 — Write your code here\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.feature import Tokenizer, CountVectorizer, MinHashLSH\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# (a) Create DataFrame, tokenize, vectorize\n",
        "docs = [\n",
        "    (1, \"the cat sat on the mat\"),\n",
        "    (2, \"the cat sat on the hat\"),\n",
        "    (3, \"the dog ran in the park\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(docs, [\"id\", \"text\"])\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "df_tokens = tokenizer.transform(df)\n",
        "\n",
        "# CountVectorizer (binary=True)\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", binary=True)\n",
        "cv_model = cv.fit(df_tokens)\n",
        "df_features = cv_model.transform(df_tokens)\n",
        "\n",
        "print(\"Schema after vectorization:\")\n",
        "df_features.printSchema()\n",
        "\n",
        "\n",
        "# (b) Fit MinHashLSH and show hashes\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3)\n",
        "mh_model = mh.fit(df_features)\n",
        "\n",
        "df_hashed = mh_model.transform(df_features)\n",
        "\n",
        "print(\"Hashed Data:\")\n",
        "df_hashed.select(\"id\", \"hashes\").show(truncate=False)\n",
        "\n",
        "\n",
        "# (c) approxSimilarityJoin with threshold 0.6\n",
        "similar_pairs = mh_model.approxSimilarityJoin(\n",
        "    df_features, df_features, 0.6, distCol=\"JaccardDistance\"\n",
        ").filter(col(\"datasetA.id\") < col(\"datasetB.id\"))\n",
        "\n",
        "print(\"Similar document pairs (threshold=0.6):\")\n",
        "similar_pairs.select(\n",
        "    col(\"datasetA.id\").alias(\"doc1\"),\n",
        "    col(\"datasetB.id\").alias(\"doc2\"),\n",
        "    \"JaccardDistance\"\n",
        ").show()\n",
        "\n",
        "\n",
        "# (d) approxNearestNeighbors for document A (id=1)\n",
        "doc_a = df_features.filter(col(\"id\") == 1).select(\"features\").first()[\"features\"]\n",
        "\n",
        "nearest = mh_model.approxNearestNeighbors(\n",
        "    df_features, doc_a, 2, distCol=\"JaccardDistance\"\n",
        ")\n",
        "\n",
        "print(\"2 Nearest Neighbours of Document A:\")\n",
        "nearest.select(\"id\", \"JaccardDistance\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped. Practice quiz complete!\n"
          ]
        }
      ],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped. Practice quiz complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### End of Practice Quiz\n",
        "\n",
        "**Review your answers and check:**\n",
        "- [ ] All code cells execute without errors\n",
        "- [ ] Outputs match what you expect\n",
        "- [ ] You understand the concepts tested\n",
        "\n",
        "---\n",
        "*SCC.454: Large Scale Platforms for AI and Data Analysis — Lancaster University*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
