{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bb372f",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0d64b",
   "metadata": {},
   "source": [
    "## Installing PySpark and OpenJDK and Setting Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb97e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark and Java installed successfully!\n",
      "d:\\Lancaster University Coursework\\Term 2\\SSC 454 - Large scale platforms for AI and Data Analysis\\Labs\\venv\\Scripts\\python.exe\n",
      "Spark Version: 3.5.0\n",
      "Application Name: SCC454-DataPreprocessing\n",
      "Master: local[*]\n",
      "\n",
      "Spark Session ready for data preprocessing!\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark\n",
    "# !pip install pyspark==3.5.0 -q\n",
    "# Install Java (Spark requires Java)\n",
    "# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "# Set Java environment variable\n",
    "import os, sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-17\" # For windows, for mac keep commented\n",
    "print(\"PySpark and Java installed successfully!\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "print(sys.executable)\n",
    "\n",
    "# Create a SparkSession configured for data preprocessing\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SCC454-DataPreprocessing\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .config(\"spark.ui.port\", \"4050\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# Get the underlying SparkContext\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(\"\\nSpark Session ready for data preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b592554",
   "metadata": {},
   "source": [
    "## Spark Data Cleaning Capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a517ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All preprocessing functions imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import commonly used functions for data preprocessing\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    isnan,\n",
    "    isnull,\n",
    "    coalesce,\n",
    "    lower,\n",
    "    upper,\n",
    "    trim,\n",
    "    ltrim,\n",
    "    rtrim,\n",
    "    length,\n",
    "    split,\n",
    "    concat,\n",
    "    concat_ws,\n",
    "    substring,\n",
    "    replace,\n",
    "    regexp_extract,\n",
    "    regexp_replace,\n",
    "    to_date,\n",
    "    to_timestamp,\n",
    "    count,\n",
    "    avg,\n",
    "    sum as spark_sum,\n",
    "    mean,\n",
    "    stddev,\n",
    "    min as spark_min,\n",
    "    max as spark_max,\n",
    "    when,\n",
    "    expr,\n",
    "    year,\n",
    "    month,\n",
    "    dayofmonth,\n",
    "    datediff,\n",
    "    date_format,\n",
    "    monotonically_increasing_id,\n",
    "    round as spark_round,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    BooleanType,\n",
    ")\n",
    "\n",
    "print(\"All preprocessing functions imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0207d8",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7915a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Data with Missing Values:\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "|customer_id|name        |age |email            |salary |city       |signup_date|\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "|1          |John Smith  |28  |john@email.com   |75000.0|New York   |2023-01-15 |\n",
      "|2          |Jane Doe    |NULL|jane@email.com   |82000.0|Los Angeles|2023-02-20 |\n",
      "|3          |Bob Wilson  |35  |NULL             |65000.0|Chicago    |2023-03-10 |\n",
      "|4          |Alice Brown |42  |alice@email.com  |NULL   |Houston    |NULL       |\n",
      "|5          |NULL        |31  |unknown@email.com|70000.0|Phoenix    |2023-05-01 |\n",
      "|6          |Charlie Lee |29  |                 |58000.0|           |2023-06-15 |\n",
      "|7          |Diana Prince|NULL|diana@email.com  |95000.0|NULL       |2023-07-20 |\n",
      "|8          |Edward Kim  |38  |N/A              |N/A    |Boston     |2023-08-25 |\n",
      "|9          |Fiona Garcia|45  |fiona@email.com  |88000.0|Seattle    |2023-09-30 |\n",
      "|10         |George Hall |NULL|NULL             |NULL   |NULL       |NULL       |\n",
      "|11         |Hannah White|33  |hannah@email.com |72000.0|Denver     |2023-11-10 |\n",
      "|12         |Ivan Torres |27  |ivan@email.com   |63000.0|Miami      |2023-12-05 |\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with various missing value patterns\n",
    "customer_data = [\n",
    "    (1, \"John Smith\", 28, \"john@email.com\", 75000.0, \"New York\", \"2023-01-15\"),\n",
    "    (2, \"Jane Doe\", None, \"jane@email.com\", 82000.0, \"Los Angeles\", \"2023-02-20\"),\n",
    "    (3, \"Bob Wilson\", 35, None, 65000.0, \"Chicago\", \"2023-03-10\"),\n",
    "    (4, \"Alice Brown\", 42, \"alice@email.com\", None, \"Houston\", None),\n",
    "    (5, None, 31, \"unknown@email.com\", 70000.0, \"Phoenix\", \"2023-05-01\"),\n",
    "    (6, \"Charlie Lee\", 29, \"\", 58000.0, \"\", \"2023-06-15\"),\n",
    "    (7, \"Diana Prince\", None, \"diana@email.com\", 95000.0, None, \"2023-07-20\"),\n",
    "    (8, \"Edward Kim\", 38, \"N/A\", \"N/A\", \"Boston\", \"2023-08-25\"),\n",
    "    (9, \"Fiona Garcia\", 45, \"fiona@email.com\", 88000.0, \"Seattle\", \"2023-09-30\"),\n",
    "    (10, \"George Hall\", None, None, None, None, None),\n",
    "    (11, \"Hannah White\", 33, \"hannah@email.com\", 72000.0, \"Denver\", \"2023-11-10\"),\n",
    "    (12, \"Ivan Torres\", 27, \"ivan@email.com\", 63000.0, \"Miami\", \"2023-12-05\"),\n",
    "]\n",
    "columns = [\"customer_id\", \"name\", \"age\", \"email\", \"salary\", \"city\", \"signup_date\"]\n",
    "df_customers = spark.createDataFrame(customer_data, columns)\n",
    "print(\"Customer Data with Missing Values:\")\n",
    "df_customers.show(truncate=False)\n",
    "# Check the schema\n",
    "print(\"Schema:\")\n",
    "df_customers.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b06b0",
   "metadata": {},
   "source": [
    "### Count Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3036bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts per column:\n",
      "+-----------+----+---+-----+------+----+-----------+\n",
      "|customer_id|name|age|email|salary|city|signup_date|\n",
      "+-----------+----+---+-----+------+----+-----------+\n",
      "|          0|   1|  3|    2|     2|   2|          2|\n",
      "+-----------+----+---+-----+------+----+-----------+\n",
      "\n",
      "Null + NaN counts for numeric columns:\n",
      "+--------------+\n",
      "|salary_missing|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE MISSING VALUE REPORT\n",
      "============================================================\n",
      "+-----------+------------+-----+-----+---+-------------+---------+\n",
      "|Column     |Type        |Nulls|Empty|NaN|Total_Missing|Missing_%|\n",
      "+-----------+------------+-----+-----+---+-------------+---------+\n",
      "|signup_date|StringType()|2    |0    |0  |2            |16.67    |\n",
      "+-----------+------------+-----+-----+---+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Count nulls per column\n",
    "print(\"Null counts per column:\")\n",
    "null_counts = df_customers.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df_customers.columns]\n",
    ")\n",
    "null_counts.show()\n",
    "\n",
    "# Method 2: Count nulls AND NaN values (important for numeric columns)\n",
    "print(\"Null + NaN counts for numeric columns:\")\n",
    "df_customers.select(\n",
    "    [\n",
    "        count(when(col(\"salary\").isNull() | isnan(col(\"salary\")), 1)).alias(\n",
    "            \"salary_missing\"\n",
    "        )\n",
    "    ]\n",
    ").show()\n",
    "\n",
    "\n",
    "# Method 3: Comprehensive missing value report\n",
    "def missing_value_report(df):\n",
    "    total_rows = df.count()\n",
    "    report_data = []\n",
    "    for col_name in df.columns:\n",
    "        col_type = str(df.schema[col_name].dataType)\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    empty_count = 0\n",
    "    if \"String\" in col_type:\n",
    "        empty_count = df.filter(\n",
    "            (col(col_name) == \"\")\n",
    "            | (col(col_name) == \"N/A\")\n",
    "            | (col(col_name) == \"Unknown\")\n",
    "        ).count()\n",
    "    nan_count = 0\n",
    "    if \"Double\" in col_type or \"Float\" in col_type:\n",
    "        nan_count = df.filter(isnan(col(col_name))).count()\n",
    "    total_missing = null_count + empty_count + nan_count\n",
    "    missing_pct = (total_missing / total_rows) * 100\n",
    "    report_data.append(\n",
    "        (\n",
    "            col_name,\n",
    "            col_type,\n",
    "            null_count,\n",
    "            empty_count,\n",
    "            nan_count,\n",
    "            total_missing,\n",
    "            round(missing_pct, 2),\n",
    "        )\n",
    "    )\n",
    "    report_columns = [\n",
    "        \"Column\",\n",
    "        \"Type\",\n",
    "        \"Nulls\",\n",
    "        \"Empty\",\n",
    "        \"NaN\",\n",
    "        \"Total_Missing\",\n",
    "        \"Missing_%\",\n",
    "    ]\n",
    "    return spark.createDataFrame(report_data, report_columns)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE MISSING VALUE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "report = missing_value_report(df_customers)\n",
    "report.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018557e",
   "metadata": {},
   "source": [
    "### Drop Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b9a8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count: 12\n",
      "After dropping rows with ANY null: 6\n",
      "+-----------+------------+---+----------------+-------+--------+-----------+\n",
      "|customer_id|        name|age|           email| salary|    city|signup_date|\n",
      "+-----------+------------+---+----------------+-------+--------+-----------+\n",
      "|          1|  John Smith| 28|  john@email.com|75000.0|New York| 2023-01-15|\n",
      "|          6| Charlie Lee| 29|                |58000.0|        | 2023-06-15|\n",
      "|          8|  Edward Kim| 38|             N/A|    N/A|  Boston| 2023-08-25|\n",
      "|          9|Fiona Garcia| 45| fiona@email.com|88000.0| Seattle| 2023-09-30|\n",
      "|         11|Hannah White| 33|hannah@email.com|72000.0|  Denver| 2023-11-10|\n",
      "|         12| Ivan Torres| 27|  ivan@email.com|63000.0|   Miami| 2023-12-05|\n",
      "+-----------+------------+---+----------------+-------+--------+-----------+\n",
      "\n",
      "After dropping rows where ALL are null:12\n",
      "After dropping rows where name OR email is null:9\n",
      "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
      "|customer_id|        name| age|           email| salary|       city|signup_date|\n",
      "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
      "|          1|  John Smith|  28|  john@email.com|75000.0|   New York| 2023-01-15|\n",
      "|          2|    Jane Doe|NULL|  jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
      "|          4| Alice Brown|  42| alice@email.com|   NULL|    Houston|       NULL|\n",
      "|          6| Charlie Lee|  29|                |58000.0|           | 2023-06-15|\n",
      "|          7|Diana Prince|NULL| diana@email.com|95000.0|       NULL| 2023-07-20|\n",
      "|          8|  Edward Kim|  38|             N/A|    N/A|     Boston| 2023-08-25|\n",
      "|          9|Fiona Garcia|  45| fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
      "|         11|Hannah White|  33|hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
      "|         12| Ivan Torres|  27|  ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
      "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
      "\n",
      "After dropping rows with < 5 non-null values:11\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "|          1|  John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
      "|          2|    Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
      "|          3|  Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
      "|          4| Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
      "|          5|        NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
      "|          6| Charlie Lee|  29|                 |58000.0|           | 2023-06-15|\n",
      "|          7|Diana Prince|NULL|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
      "|          8|  Edward Kim|  38|              N/A|    N/A|     Boston| 2023-08-25|\n",
      "|          9|Fiona Garcia|  45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
      "|         11|Hannah White|  33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
      "|         12| Ivan Torres|  27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
      "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original count\n",
    "print(f\"Original row count: {df_customers.count()}\")\n",
    "\n",
    "# Drop rows with ANY null value (most aggressive)\n",
    "df_drop_any = df_customers.na.drop(how=\"any\")\n",
    "print(f\"After dropping rows with ANY null: {df_drop_any.count()}\")\n",
    "df_drop_any.show()\n",
    "\n",
    "# Drop rows where ALL values are null (least aggressive)\n",
    "df_drop_all = df_customers.na.drop(how=\"all\")\n",
    "print(f\"After dropping rows where ALL are null:{df_drop_all.count()}\")\n",
    "\n",
    "# Drop rows where specific columns are null (most practical)\n",
    "df_drop_subset = df_customers.na.drop(subset=[\"name\", \"email\"])\n",
    "print(f\"After dropping rows where name OR email is null:{df_drop_subset.count()}\")\n",
    "df_drop_subset.show()\n",
    "\n",
    "# Drop rows with fewer than a threshold of non-null values\n",
    "df_drop_thresh = df_customers.na.drop(thresh=5)\n",
    "print(f\"After dropping rows with < 5 non-null values:{df_drop_thresh.count()}\")\n",
    "df_drop_thresh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be854348",
   "metadata": {},
   "source": [
    "### Filling Missing Values (Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a068ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill all numeric nulls with 0:\n",
      "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
      "|customer_id|        name|age|            email| salary|       city|signup_date|\n",
      "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
      "|          1|  John Smith| 28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
      "|          2|    Jane Doe|  0|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
      "|          3|  Bob Wilson| 35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
      "|          4| Alice Brown| 42|  alice@email.com|   NULL|    Houston|       NULL|\n",
      "|          5|        NULL| 31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
      "|          6| Charlie Lee| 29|                 |58000.0|           | 2023-06-15|\n",
      "|          7|Diana Prince|  0|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
      "|          8|  Edward Kim| 38|              N/A|    N/A|     Boston| 2023-08-25|\n",
      "|          9|Fiona Garcia| 45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
      "|         10| George Hall|  0|             NULL|   NULL|       NULL|       NULL|\n",
      "|         11|Hannah White| 33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
      "|         12| Ivan Torres| 27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
      "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
      "\n",
      "Fill with specific values per column:\n",
      "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
      "|customer_id|name        |age|email                   |salary |city       |signup_date|\n",
      "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
      "|1          |John Smith  |28 |john@email.com          |75000.0|New York   |2023-01-15 |\n",
      "|2          |Jane Doe    |30 |jane@email.com          |82000.0|Los Angeles|2023-02-20 |\n",
      "|3          |Bob Wilson  |35 |no_email@placeholder.com|65000.0|Chicago    |2023-03-10 |\n",
      "|4          |Alice Brown |42 |alice@email.com         |0.0    |Houston    |1970-01-01 |\n",
      "|5          |Unknown     |31 |unknown@email.com       |70000.0|Phoenix    |2023-05-01 |\n",
      "|6          |Charlie Lee |29 |                        |58000.0|           |2023-06-15 |\n",
      "|7          |Diana Prince|30 |diana@email.com         |95000.0|Unknown    |2023-07-20 |\n",
      "|8          |Edward Kim  |38 |N/A                     |N/A    |Boston     |2023-08-25 |\n",
      "|9          |Fiona Garcia|45 |fiona@email.com         |88000.0|Seattle    |2023-09-30 |\n",
      "|10         |George Hall |30 |no_email@placeholder.com|0.0    |Unknown    |1970-01-01 |\n",
      "|11         |Hannah White|33 |hannah@email.com        |72000.0|Denver     |2023-11-10 |\n",
      "|12         |Ivan Torres |27 |ivan@email.com          |63000.0|Miami      |2023-12-05 |\n",
      "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill all numeric columns with a single value\n",
    "print(\"Fill all numeric nulls with 0:\")\n",
    "df_customers.na.fill(0).show()\n",
    "\n",
    "# Fill specific columns with specific values (dictionary approach)\n",
    "fill_values = {\n",
    "    \"name\": \"Unknown\",\n",
    "    \"age\": 30,\n",
    "    \"email\": \"no_email@placeholder.com\",\n",
    "    \"salary\": 0.0,\n",
    "    \"city\": \"Unknown\",\n",
    "    \"signup_date\": \"1970-01-01\",\n",
    "}\n",
    "print(\"Fill with specific values per column:\")\n",
    "df_filled = df_customers.na.fill(fill_values)\n",
    "df_filled.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d08694",
   "metadata": {},
   "source": [
    "#### Statistical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9a0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age: 34.22\n",
      "Median age: 33\n",
      "\n",
      "Age filled with mean:\n",
      "+-----------+------------+---+\n",
      "|customer_id|        name|age|\n",
      "+-----------+------------+---+\n",
      "|          1|  John Smith| 28|\n",
      "|          2|    Jane Doe| 34|\n",
      "|          3|  Bob Wilson| 35|\n",
      "|          4| Alice Brown| 42|\n",
      "|          5|        NULL| 31|\n",
      "|          6| Charlie Lee| 29|\n",
      "|          7|Diana Prince| 34|\n",
      "|          8|  Edward Kim| 38|\n",
      "|          9|Fiona Garcia| 45|\n",
      "|         10| George Hall| 34|\n",
      "|         11|Hannah White| 33|\n",
      "|         12| Ivan Torres| 27|\n",
      "+-----------+------------+---+\n",
      "\n",
      "Mean salary: $74,222.22\n",
      "Median salary: $72,000.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean for age column (excluding nulls)\n",
    "age_stats = df_customers.select(\n",
    "    mean(col(\"age\")).alias(\"mean_age\"),\n",
    "    expr(\"percentile_approx(age, 0.5)\").alias(\"median_age\"),\n",
    ").collect()[0]\n",
    "mean_age = age_stats[\"mean_age\"]\n",
    "median_age = age_stats[\"median_age\"]\n",
    "print(f\"Mean age: {mean_age:.2f}\")\n",
    "print(f\"Median age: {median_age}\")\n",
    "\n",
    "# Fill with mean\n",
    "df_mean_imputed = df_customers.na.fill({\"age\": int(mean_age)})\n",
    "print(\"\\nAge filled with mean:\")\n",
    "df_mean_imputed.select(\"customer_id\", \"name\", \"age\").show()\n",
    "\n",
    "# Calculate and impute with median for salary\n",
    "salary_stats = (\n",
    "    df_customers.filter(col(\"salary\").isNotNull() & ~isnan(col(\"salary\")))\n",
    "    .select(\n",
    "        mean(col(\"salary\")).alias(\"mean_salary\"),\n",
    "        expr(\"percentile_approx(salary, 0.5)\").alias(\"median_salary\"),\n",
    "    )\n",
    "    .collect()[0]\n",
    ")\n",
    "print(f\"Mean salary: ${salary_stats['mean_salary']:,.2f}\")\n",
    "print(f\"Median salary: ${salary_stats['median_salary']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8a620",
   "metadata": {},
   "source": [
    "### Advanced Imputation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa59e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average salary per city:\n",
      "+-----------+---------------+\n",
      "|       city|city_avg_salary|\n",
      "+-----------+---------------+\n",
      "|   New York|        75000.0|\n",
      "|Los Angeles|        82000.0|\n",
      "|    Chicago|        65000.0|\n",
      "|    Phoenix|        70000.0|\n",
      "|           |        58000.0|\n",
      "|     Boston|           NULL|\n",
      "|    Seattle|        88000.0|\n",
      "|     Denver|        72000.0|\n",
      "|      Miami|        63000.0|\n",
      "+-----------+---------------+\n",
      "\n",
      "Smart imputation using city averages:\n",
      "+-----------+------------+-----------+-------+---------------+-----------------+\n",
      "|customer_id|        name|       city| salary|city_avg_salary|   salary_imputed|\n",
      "+-----------+------------+-----------+-------+---------------+-----------------+\n",
      "|          1|  John Smith|   New York|75000.0|        75000.0|          75000.0|\n",
      "|          2|    Jane Doe|Los Angeles|82000.0|        82000.0|          82000.0|\n",
      "|          3|  Bob Wilson|    Chicago|65000.0|        65000.0|          65000.0|\n",
      "|          4| Alice Brown|    Houston|   NULL|           NULL|74222.22222222222|\n",
      "|          5|        NULL|    Phoenix|70000.0|        70000.0|          70000.0|\n",
      "|          6| Charlie Lee|           |58000.0|        58000.0|          58000.0|\n",
      "|          7|Diana Prince|       NULL|95000.0|           NULL|          95000.0|\n",
      "|          8|  Edward Kim|     Boston|    N/A|           NULL|              N/A|\n",
      "|          9|Fiona Garcia|    Seattle|88000.0|        88000.0|          88000.0|\n",
      "|         10| George Hall|       NULL|   NULL|           NULL|74222.22222222222|\n",
      "|         11|Hannah White|     Denver|72000.0|        72000.0|          72000.0|\n",
      "|         12| Ivan Torres|      Miami|63000.0|        63000.0|          63000.0|\n",
      "+-----------+------------+-----------+-------+---------------+-----------------+\n",
      "\n",
      "Time series with missing values:\n",
      "+----------+-----+\n",
      "|      date|value|\n",
      "+----------+-----+\n",
      "|2024-01-01|100.0|\n",
      "|2024-01-02| NULL|\n",
      "|2024-01-03| NULL|\n",
      "|2024-01-04|105.0|\n",
      "|2024-01-05| NULL|\n",
      "|2024-01-06|108.0|\n",
      "|2024-01-07| NULL|\n",
      "+----------+-----+\n",
      "\n",
      "After forward fill:\n",
      "+----------+-----+-----------+\n",
      "|      date|value|value_ffill|\n",
      "+----------+-----+-----------+\n",
      "|2024-01-01|100.0|      100.0|\n",
      "|2024-01-02| NULL|      100.0|\n",
      "|2024-01-03| NULL|      100.0|\n",
      "|2024-01-04|105.0|      105.0|\n",
      "|2024-01-05| NULL|      105.0|\n",
      "|2024-01-06|108.0|      108.0|\n",
      "|2024-01-07| NULL|      108.0|\n",
      "+----------+-----+-----------+\n",
      "\n",
      "Data with missing value indicators:\n",
      "+-----------+------------+----+---------------+-------+------------------+\n",
      "|customer_id|        name| age|age_was_missing| salary|salary_was_missing|\n",
      "+-----------+------------+----+---------------+-------+------------------+\n",
      "|          1|  John Smith|  28|              0|75000.0|                 0|\n",
      "|          2|    Jane Doe|NULL|              1|82000.0|                 0|\n",
      "|          3|  Bob Wilson|  35|              0|65000.0|                 0|\n",
      "|          4| Alice Brown|  42|              0|   NULL|                 1|\n",
      "|          5|        NULL|  31|              0|70000.0|                 0|\n",
      "|          6| Charlie Lee|  29|              0|58000.0|                 0|\n",
      "|          7|Diana Prince|NULL|              1|95000.0|                 0|\n",
      "|          8|  Edward Kim|  38|              0|    N/A|                 0|\n",
      "|          9|Fiona Garcia|  45|              0|88000.0|                 0|\n",
      "|         10| George Hall|NULL|              1|   NULL|                 1|\n",
      "|         11|Hannah White|  33|              0|72000.0|                 0|\n",
      "|         12| Ivan Torres|  27|              0|63000.0|                 0|\n",
      "+-----------+------------+----+---------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group-wise imputation: Fill missing values based on group statistics\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate average salary per city\n",
    "city_avg_salary = (\n",
    "    df_customers.filter(\n",
    "        col(\"salary\").isNotNull() & ~isnan(col(\"salary\")) & col(\"city\").isNotNull()\n",
    "    )\n",
    "    .groupBy(\"city\")\n",
    "    .agg(spark_round(mean(\"salary\"), 2).alias(\"city_avg_salary\"))\n",
    ")\n",
    "print(\"Average salary per city:\")\n",
    "city_avg_salary.show()\n",
    "\n",
    "# Using coalesce for conditional filling\n",
    "overall_avg_salary = salary_stats[\"mean_salary\"]\n",
    "df_smart_imputed = (\n",
    "    df_customers.join(city_avg_salary, on=\"city\", how=\"left\")\n",
    "    .withColumn(\n",
    "        \"salary_imputed\",\n",
    "        coalesce(\n",
    "            when(~isnan(col(\"salary\")), col(\"salary\")),\n",
    "            col(\"city_avg_salary\"),\n",
    "            lit(overall_avg_salary),\n",
    "        ),\n",
    "    )\n",
    "    .select(\n",
    "        \"customer_id\", \"name\", \"city\", \"salary\", \"city_avg_salary\", \"salary_imputed\"\n",
    "    )\n",
    ")\n",
    "print(\"Smart imputation using city averages:\")\n",
    "df_smart_imputed.show()\n",
    "\n",
    "# Forward Fill using Window functions (useful for time-series)\n",
    "from pyspark.sql.functions import last, first\n",
    "\n",
    "ts_data = [\n",
    "    (\"2024-01-01\", 100.0),\n",
    "    (\"2024-01-02\", None),\n",
    "    (\"2024-01-03\", None),\n",
    "    (\"2024-01-04\", 105.0),\n",
    "    (\"2024-01-05\", None),\n",
    "    (\"2024-01-06\", 108.0),\n",
    "    (\"2024-01-07\", None),\n",
    "]\n",
    "df_ts = spark.createDataFrame(ts_data, [\"date\", \"value\"])\n",
    "print(\"Time series with missing values:\")\n",
    "df_ts.show()\n",
    "\n",
    "# Forward fill\n",
    "window_forward = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df_forward_fill = df_ts.withColumn(\n",
    "    \"value_ffill\", last(col(\"value\"), ignorenulls=True).over(window_forward)\n",
    ")\n",
    "print(\"After forward fill:\")\n",
    "df_forward_fill.show()\n",
    "\n",
    "# Creating missing value indicator columns\n",
    "df_with_indicators = df_customers.withColumn(\n",
    "    \"age_was_missing\", when(col(\"age\").isNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"salary_was_missing\",\n",
    "    when(col(\"salary\").isNull() | isnan(col(\"salary\")), 1).otherwise(0),\n",
    ")\n",
    "print(\"Data with missing value indicators:\")\n",
    "df_with_indicators.select(\n",
    "    \"customer_id\", \"name\", \"age\", \"age_was_missing\", \"salary\", \"salary_was_missing\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fc15d",
   "metadata": {},
   "source": [
    "## Understanding Spark Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc6e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messy data with type issues:\n",
      "+---+------+--------+--------------+------+\n",
      "| id|   age|  salary|          date|active|\n",
      "+---+------+--------+--------------+------+\n",
      "|  1|    25|50000.50|    2024-01-15|  true|\n",
      "|  2|thirty|   60000|    15/02/2024|   yes|\n",
      "|  3|    35| $75,000|    2024-03-20|     1|\n",
      "|  4|    40|80000.00|March 25, 2024| false|\n",
      "|  5|   N/A| invalid|    2024-04-30|    no|\n",
      "+---+------+--------+--------------+------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- active: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with mixed type issues\n",
    "messy_data = [\n",
    "    (\"1\", \"25\", \"50000.50\", \"2024-01-15\", \"true\"),\n",
    "    (\"2\", \"thirty\", \"60000\", \"15/02/2024\", \"yes\"),\n",
    "    (\"3\", \"35\", \"$75,000\", \"2024-03-20\", \"1\"),\n",
    "    (\"4\", \"40\", \"80000.00\", \"March 25, 2024\", \"false\"),\n",
    "    (\"5\", \"N/A\", \"invalid\", \"2024-04-30\", \"no\"),\n",
    "]\n",
    "df_messy = spark.createDataFrame(messy_data, [\"id\", \"age\", \"salary\", \"date\", \"active\"])\n",
    "print(\"Messy data with type issues:\")\n",
    "df_messy.show()\n",
    "df_messy.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cef703",
   "metadata": {},
   "source": [
    "### Type Casting and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID cast to integer:\n",
      "+---+------+\n",
      "| id|id_int|\n",
      "+---+------+\n",
      "|  1|     1|\n",
      "|  2|     2|\n",
      "|  3|     3|\n",
      "|  4|     4|\n",
      "|  5|     5|\n",
      "+---+------+\n",
      "\n",
      "Age cast to integer (note nulls for invalid values):\n",
      "+------+-------+\n",
      "|   age|age_int|\n",
      "+------+-------+\n",
      "|    25|     25|\n",
      "|thirty|   NULL|\n",
      "|    35|     35|\n",
      "|    40|     40|\n",
      "|   N/A|   NULL|\n",
      "+------+-------+\n",
      "\n",
      "Salary cleaning and casting:\n",
      "+--------+--------------+-------------+\n",
      "|  salary|salary_cleaned|salary_double|\n",
      "+--------+--------------+-------------+\n",
      "|50000.50|      50000.50|      50000.5|\n",
      "|   60000|         60000|      60000.0|\n",
      "| $75,000|         75000|      75000.0|\n",
      "|80000.00|      80000.00|      80000.0|\n",
      "| invalid|       invalid|         NULL|\n",
      "+--------+--------------+-------------+\n",
      "\n",
      "Boolean conversion:\n",
      "+------+-----------+\n",
      "|active|active_bool|\n",
      "+------+-----------+\n",
      "|  true|       true|\n",
      "|   yes|       true|\n",
      "|     1|       true|\n",
      "| false|      false|\n",
      "|    no|      false|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple casting\n",
    "df_cast = df_messy.withColumn(\"id_int\", col(\"id\").cast(IntegerType()))\n",
    "print(\"ID cast to integer:\")\n",
    "df_cast.select(\"id\", \"id_int\").show()\n",
    "\n",
    "# Casting with potential failures - non-numeric strings become null\n",
    "df_age_cast = df_messy.withColumn(\"age_int\", col(\"age\").cast(IntegerType()))\n",
    "print(\"Age cast to integer (note nulls for invalid values):\")\n",
    "df_age_cast.select(\"age\", \"age_int\").show()\n",
    "\n",
    "# Clean salary string before casting\n",
    "df_salary_clean = df_messy.withColumn(\n",
    "    \"salary_cleaned\", regexp_replace(col(\"salary\"), \"[$,]\", \"\")\n",
    ").withColumn(\"salary_double\", col(\"salary_cleaned\").cast(DoubleType()))\n",
    "print(\"Salary cleaning and casting:\")\n",
    "df_salary_clean.select(\"salary\", \"salary_cleaned\", \"salary_double\").show()\n",
    "\n",
    "# Boolean conversion with multiple representations\n",
    "df_bool = df_messy.withColumn(\n",
    "    \"active_bool\",\n",
    "    when(lower(col(\"active\")).isin(\"true\", \"yes\", \"1\", \"t\", \"y\"), True)\n",
    "    .when(lower(col(\"active\")).isin(\"false\", \"no\", \"0\", \"f\", \"n\"), False)\n",
    "    .otherwise(None),\n",
    ")\n",
    "print(\"Boolean conversion:\")\n",
    "df_bool.select(\"active\", \"active_bool\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2b01b",
   "metadata": {},
   "source": [
    "### Handling Date and Timestamp Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|date_string   |format_name        |\n",
      "+--------------+-------------------+\n",
      "|2024-01-15    |ISO format         |\n",
      "|15/02/2024    |European DD/MM/YYYY|\n",
      "|03/20/2024    |American MM/DD/YYYY|\n",
      "|March 25, 2024|Written format     |\n",
      "|2024.04.30    |Dot separator      |\n",
      "+--------------+-------------------+\n",
      "\n",
      "Dates parsed from multiple formats:\n",
      "+--------------+-------------------+-----------+\n",
      "|date_string   |format_name        |parsed_date|\n",
      "+--------------+-------------------+-----------+\n",
      "|2024-01-15    |ISO format         |2024-01-15 |\n",
      "|15/02/2024    |European DD/MM/YYYY|2024-02-15 |\n",
      "|03/20/2024    |American MM/DD/YYYY|2024-03-20 |\n",
      "|March 25, 2024|Written format     |2024-03-25 |\n",
      "|2024.04.30    |Dot separator      |2024-04-30 |\n",
      "+--------------+-------------------+-----------+\n",
      "\n",
      "Date components extracted:\n",
      "+--------------+-----------+----+-----+---+-------+---------+\n",
      "|date_string   |parsed_date|year|month|day|quarter|day_name |\n",
      "+--------------+-----------+----+-----+---+-------+---------+\n",
      "|2024-01-15    |2024-01-15 |2024|1    |15 |1      |Monday   |\n",
      "|15/02/2024    |2024-02-15 |2024|2    |15 |1      |Thursday |\n",
      "|03/20/2024    |2024-03-20 |2024|3    |20 |1      |Wednesday|\n",
      "|March 25, 2024|2024-03-25 |2024|3    |25 |1      |Monday   |\n",
      "|2024.04.30    |2024-04-30 |2024|4    |30 |2      |Tuesday  |\n",
      "+--------------+-----------+----+-----+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common date formats\n",
    "date_examples = [\n",
    "    (\"2024-01-15\", \"ISO format\"),\n",
    "    (\"15/02/2024\", \"European DD/MM/YYYY\"),\n",
    "    (\"03/20/2024\", \"American MM/DD/YYYY\"),\n",
    "    (\"March 25, 2024\", \"Written format\"),\n",
    "    (\"2024.04.30\", \"Dot separator\"),\n",
    "]\n",
    "df_dates = spark.createDataFrame(date_examples, [\"date_string\", \"format_name\"])\n",
    "df_dates.show(truncate=False)\n",
    "\n",
    "# Parse dates with multiple format patterns using coalesce\n",
    "df_parsed = df_dates.withColumn(\n",
    "    \"parsed_date\",\n",
    "    coalesce(\n",
    "        to_date(col(\"date_string\"), \"yyyy-MM-dd\"),\n",
    "        to_date(col(\"date_string\"), \"dd/MM/yyyy\"),\n",
    "        to_date(col(\"date_string\"), \"MM/dd/yyyy\"),\n",
    "        to_date(col(\"date_string\"), \"MMMM dd, yyyy\"),\n",
    "        to_date(col(\"date_string\"), \"yyyy.MM.dd\"),\n",
    "    ),\n",
    ")\n",
    "print(\"Dates parsed from multiple formats:\")\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "# Extract components from dates\n",
    "from pyspark.sql.functions import (\n",
    "    year,\n",
    "    month,\n",
    "    dayofmonth,\n",
    "    dayofweek,\n",
    "    quarter,\n",
    "    date_format,\n",
    ")\n",
    "\n",
    "df_date_parts = df_parsed.filter(col(\"parsed_date\").isNotNull()).select(\n",
    "    col(\"date_string\"),\n",
    "    col(\"parsed_date\"),\n",
    "    year(col(\"parsed_date\")).alias(\"year\"),\n",
    "    month(col(\"parsed_date\")).alias(\"month\"),\n",
    "    dayofmonth(col(\"parsed_date\")).alias(\"day\"),\n",
    "    quarter(col(\"parsed_date\")).alias(\"quarter\"),\n",
    "    date_format(col(\"parsed_date\"), \"EEEE\").alias(\"day_name\"),\n",
    ")\n",
    "print(\"Date components extracted:\")\n",
    "df_date_parts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0842a75",
   "metadata": {},
   "source": [
    "### Schema Validation and Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdc25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Schema:\n",
      " customer_id: IntegerType()(nullable=False)\n",
      " name: StringType()(nullable=True)\n",
      " age: IntegerType()(nullable=True)\n",
      " email: StringType()(nullable=True)\n",
      " salary: DoubleType()(nullable=True)\n",
      " city: StringType()(nullable=True)\n",
      " signup_date: DateType()(nullable=True)\n",
      "Schema-enforced DataFrame:\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n",
      "+-----------+-----------+----+-----------------+-------+-----------+-----------+\n",
      "|customer_id|       name| age|            email| salary|       city|signup_date|\n",
      "+-----------+-----------+----+-----------------+-------+-----------+-----------+\n",
      "|          1| John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
      "|          2|   Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
      "|          3| Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
      "|          4|Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
      "|          5|       NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
      "+-----------+-----------+----+-----------------+-------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an expected schema\n",
    "expected_schema = StructType(\n",
    "    [\n",
    "        StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=True),\n",
    "        StructField(\"age\", IntegerType(), nullable=True),\n",
    "        StructField(\"email\", StringType(), nullable=True),\n",
    "        StructField(\"salary\", DoubleType(), nullable=True),\n",
    "        StructField(\"city\", StringType(), nullable=True),\n",
    "        StructField(\"signup_date\", DateType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "print(\"Expected Schema:\")\n",
    "for field in expected_schema.fields:\n",
    "    print(f\" {field.name}: {field.dataType}(nullable={field.nullable})\")\n",
    "\n",
    "\n",
    "# Function to validate and transform data to match expected schema\n",
    "def enforce_schema(df, target_schema):\n",
    "    result_df = df\n",
    "    for field in target_schema.fields:\n",
    "        col_name = field.name\n",
    "    col_type = field.dataType\n",
    "    if col_name in df.columns:\n",
    "        result_df = result_df.withColumn(col_name, col(col_name).cast(col_type))\n",
    "    else:\n",
    "        result_df = result_df.withColumn(col_name, lit(None).cast(col_type))\n",
    "    return result_df.select([field.name for field in target_schema.fields])\n",
    "\n",
    "\n",
    "df_enforced = enforce_schema(df_customers, expected_schema)\n",
    "print(\"Schema-enforced DataFrame:\")\n",
    "df_enforced.printSchema()\n",
    "df_enforced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933892d4",
   "metadata": {},
   "source": [
    "## String Functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "+---+------------------+------------------------+\n",
      "|id |text              |email                   |\n",
      "+---+------------------+------------------------+\n",
      "|1  | Hello World      |john.doe@email.com      |\n",
      "|2  |APACHE SPARK      |JANE.DOE@EMAIL.COM      |\n",
      "|3  |data science      |bob_wilson@company.co.uk|\n",
      "|4  | Machine Learning |alice-brown@domain.org  |\n",
      "|5  |NLP is Great!!!   |charlie.lee123@test.com |\n",
      "+---+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample text data\n",
    "text_data = [\n",
    "    (1, \" Hello World \", \"john.doe@email.com\"),\n",
    "    (2, \"APACHE SPARK\", \"JANE.DOE@EMAIL.COM\"),\n",
    "    (3, \"data science\", \"bob_wilson@company.co.uk\"),\n",
    "    (4, \" Machine Learning \", \"alice-brown@domain.org\"),\n",
    "    (5, \"NLP is Great!!!\", \"charlie.lee123@test.com\"),\n",
    "]\n",
    "df_text = spark.createDataFrame(text_data, [\"id\", \"text\", \"email\"])\n",
    "print(\"Sample text data:\")\n",
    "df_text.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45819a",
   "metadata": {},
   "source": [
    "### Case Normalization and Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed0d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case conversion:\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|text              |lowercase         |uppercase         |titlecase         |\n",
      "+------------------+------------------+------------------+------------------+\n",
      "| Hello World      | hello world      | HELLO WORLD      | Hello World      |\n",
      "|APACHE SPARK      |apache spark      |APACHE SPARK      |Apache Spark      |\n",
      "|data science      |data science      |DATA SCIENCE      |Data Science      |\n",
      "| Machine Learning | machine learning | MACHINE LEARNING | Machine Learning |\n",
      "|NLP is Great!!!   |nlp is great!!!   |NLP IS GREAT!!!   |Nlp Is Great!!!   |\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n",
      "Whitespace trimming:\n",
      "+------------------+----------------+-----------------+-----------------+------------+-----------+\n",
      "|text              |trimmed         |left_trimmed     |right_trimmed    |original_len|trimmed_len|\n",
      "+------------------+----------------+-----------------+-----------------+------------+-----------+\n",
      "| Hello World      |Hello World     |Hello World      | Hello World     |13          |11         |\n",
      "|APACHE SPARK      |APACHE SPARK    |APACHE SPARK     |APACHE SPARK     |12          |12         |\n",
      "|data science      |data science    |data science     |data science     |12          |12         |\n",
      "| Machine Learning |Machine Learning|Machine Learning | Machine Learning|18          |16         |\n",
      "|NLP is Great!!!   |NLP is Great!!! |NLP is Great!!!  |NLP is Great!!!  |15          |15         |\n",
      "+------------------+----------------+-----------------+-----------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case conversion\n",
    "df_case = df_text.select(\n",
    "    col(\"text\"),\n",
    "    lower(col(\"text\")).alias(\"lowercase\"),\n",
    "    upper(col(\"text\")).alias(\"uppercase\"),\n",
    "    expr(\"initcap(text)\").alias(\"titlecase\"),\n",
    ")\n",
    "print(\"Case conversion:\")\n",
    "df_case.show(truncate=False)\n",
    "\n",
    "# Trimming whitespace\n",
    "df_trim = df_text.select(\n",
    "    col(\"text\"),\n",
    "    trim(col(\"text\")).alias(\"trimmed\"),\n",
    "    ltrim(col(\"text\")).alias(\"left_trimmed\"),\n",
    "    rtrim(col(\"text\")).alias(\"right_trimmed\"),\n",
    "    length(col(\"text\")).alias(\"original_len\"),\n",
    "    length(trim(col(\"text\"))).alias(\"trimmed_len\"),\n",
    ")\n",
    "print(\"Whitespace trimming:\")\n",
    "df_trim.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870bc7b",
   "metadata": {},
   "source": [
    "### Tokenization and Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407df8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------+\n",
      "|id |sentence                                                          |\n",
      "+---+------------------------------------------------------------------+\n",
      "|1  |Apache Spark is a unified analytics engine for big data processing|\n",
      "|2  |Natural language processing enables computers to understand text  |\n",
      "|3  |Machine learning models require clean preprocessed data           |\n",
      "|4  |Text mining extracts valuable insights from unstructured data     |\n",
      "+---+------------------------------------------------------------------+\n",
      "\n",
      "Tokenized sentences:\n",
      "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
      "|id |sentence                                                          |words                                                                         |word_count|\n",
      "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
      "|1  |Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|11        |\n",
      "|2  |Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |8         |\n",
      "|3  |Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |7         |\n",
      "|4  |Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |8         |\n",
      "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
      "\n",
      "Word frequencies:\n",
      "+----------+---------+\n",
      "|      word|frequency|\n",
      "+----------+---------+\n",
      "|      data|        3|\n",
      "|processing|        2|\n",
      "|      text|        2|\n",
      "|       for|        1|\n",
      "|        is|        1|\n",
      "|    apache|        1|\n",
      "|     spark|        1|\n",
      "| analytics|        1|\n",
      "|         a|        1|\n",
      "|   unified|        1|\n",
      "|    engine|        1|\n",
      "|       big|        1|\n",
      "| computers|        1|\n",
      "|   natural|        1|\n",
      "|  language|        1|\n",
      "+----------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample sentences\n",
    "sentences_data = [\n",
    "    (1, \"Apache Spark is a unified analytics engine for big data processing\"),\n",
    "    (2, \"Natural language processing enables computers to understand text\"),\n",
    "    (3, \"Machine learning models require clean preprocessed data\"),\n",
    "    (4, \"Text mining extracts valuable insights from unstructured data\"),\n",
    "]\n",
    "df_sentences = spark.createDataFrame(sentences_data, [\"id\", \"sentence\"])\n",
    "df_sentences.show(truncate=False)\n",
    "\n",
    "# Basic tokenization using split\n",
    "from pyspark.sql.functions import split, size, explode\n",
    "\n",
    "df_tokens = df_sentences \\\n",
    "    .withColumn(\"words\", split(lower(col(\"sentence\")), \" \")) \\\n",
    "    .withColumn(\"word_count\", size(col(\"words\")))\n",
    "print(\"Tokenized sentences:\")\n",
    "df_tokens.show(truncate=False)\n",
    "\n",
    "# Explode tokens and calculate word frequency\n",
    "df_exploded = df_tokens.select(col(\"id\"), explode(col(\"words\")).alias(\"word\"))\n",
    "word_freq = (\n",
    "    df_exploded.groupBy(\"word\")\n",
    "    .agg(count(\"*\").alias(\"frequency\"))\n",
    "    .orderBy(col(\"frequency\").desc())\n",
    ")\n",
    "print(\"Word frequencies:\")\n",
    "word_freq.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b5b50",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6080f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with stop words removed:\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|sentence                                                          |words                                                                         |words_filtered                                                       |\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|[apache, spark, unified, analytics, engine, big, data, processing]   |\n",
      "|Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |[natural, language, processing, enables, computers, understand, text]|\n",
      "|Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |[machine, learning, models, require, clean, preprocessed, data]      |\n",
      "|Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |[text, mining, extracts, valuable, insights, unstructured, data]     |\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "\n",
      "Using Spark ML StopWordsRemover:\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|sentence                                                          |words_ml                                                                      |words_cleaned                                                        |\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|[apache, spark, unified, analytics, engine, big, data, processing]   |\n",
      "|Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |[natural, language, processing, enables, computers, understand, text]|\n",
      "|Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |[machine, learning, models, require, clean, preprocessed, data]      |\n",
      "|Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |[text, mining, extracts, valuable, insights, unstructured, data]     |\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define common English stop words\n",
    "stop_words = [\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    \"but\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"to\",\n",
    "    \"of\",\n",
    "    \"in\",\n",
    "    \"for\",\n",
    "    \"on\",\n",
    "    \"with\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"from\",\n",
    "    \"it\",\n",
    "    \"its\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"be\",\n",
    "    \"been\",\n",
    "    \"being\",\n",
    "    \"have\",\n",
    "    \"has\",\n",
    "    \"had\",\n",
    "]\n",
    "from pyspark.sql.functions import array_except, array\n",
    "\n",
    "stop_words_array = array([lit(w) for w in stop_words])\n",
    "df_no_stopwords = df_tokens.withColumn(\n",
    "    \"words_filtered\", array_except(col(\"words\"), stop_words_array)\n",
    ")\n",
    "print(\"Words with stop words removed:\")\n",
    "df_no_stopwords.select(\"sentence\", \"words\", \"words_filtered\").show(truncate=False)\n",
    "\n",
    "# Using Spark ML's StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words_ml\")\n",
    "df_tokenized_ml = tokenizer.transform(df_sentences)\n",
    "remover = StopWordsRemover(inputCol=\"words_ml\", outputCol=\"words_cleaned\")\n",
    "df_cleaned_ml = remover.transform(df_tokenized_ml)\n",
    "print(\"Using Spark ML StopWordsRemover:\")\n",
    "df_cleaned_ml.select(\"sentence\", \"words_ml\", \"words_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f2e4d",
   "metadata": {},
   "source": [
    "### Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdf071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw reviews:\n",
      "+---+------------------------------------+------+\n",
      "|id |review                              |rating|\n",
      "+---+------------------------------------+------+\n",
      "|1  | GREAT product!!! Would buy again.  |5     |\n",
      "|2  |Terrible... don't waste your money  |1     |\n",
      "|3  |Good quality, fast shipping!!!      |4     |\n",
      "|4  | not bad, could be better...        |3     |\n",
      "|5  |AMAZING!!!!! Best purchase EVER!!!!!|5     |\n",
      "|6  |meh, it's OK I guess                |3     |\n",
      "|7  |Product arrived damaged             |1     |\n",
      "|8  |10/10 would recommend to friends!   |5     |\n",
      "+---+------------------------------------+------+\n",
      "\n",
      "Cleaned reviews:\n",
      "+------------------------------------+---------------------------+\n",
      "|review                              |review_cleaned             |\n",
      "+------------------------------------+---------------------------+\n",
      "| GREAT product!!! Would buy again.  |greatproductwouldbuyagain  |\n",
      "|Terrible... don't waste your money  |terribledontwasteyourmoney |\n",
      "|Good quality, fast shipping!!!      |goodqualityfastshipping    |\n",
      "| not bad, could be better...        |notbadcouldbebetter        |\n",
      "|AMAZING!!!!! Best purchase EVER!!!!!|amazingbestpurchaseever    |\n",
      "|meh, it's OK I guess                |mehitsokiguess             |\n",
      "|Product arrived damaged             |productarriveddamaged      |\n",
      "|10/10 would recommend to friends!   |1010wouldrecommendtofriends|\n",
      "+------------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample product reviews (messy text data)\n",
    "reviews_data = [\n",
    "    (1, \" GREAT product!!! Would buy again. \", 5),\n",
    "    (2, \"Terrible... don't waste your money \", 1),\n",
    "    (3, \"Good quality, fast shipping!!!\", 4),\n",
    "    (4, \" not bad, could be better... \", 3),\n",
    "    (5, \"AMAZING!!!!! Best purchase EVER!!!!!\", 5),\n",
    "    (6, \"meh, it's OK I guess\", 3),\n",
    "    (7, \"Product arrived damaged\", 1),\n",
    "    (8, \"10/10 would recommend to friends!\", 5),\n",
    "]\n",
    "df_reviews = spark.createDataFrame(reviews_data, [\"id\", \"review\", \"rating\"])\n",
    "print(\"Raw reviews:\")\n",
    "df_reviews.show(truncate=False)\n",
    "\n",
    "\n",
    "def clean_text_pipeline(df, text_col, output_col=\"text_cleaned\"):\n",
    "    result = (\n",
    "        df.withColumn(\"_step1_trim\", trim(col(text_col)))\n",
    "        .withColumn(\"_step2_lower\", lower(col(\"_step1_trim\")))\n",
    "        .withColumn(\n",
    "            \"_step3_alphanum\", regexp_replace(col(\"_step2_lower\"), \"[^a-z0-9\\\\s]\", \"\")\n",
    "        )\n",
    "        .withColumn(\"_step4_spaces\", regexp_replace(col(\"_step3_alphanum\"), \"\\\\s+\", \"\"))\n",
    "        .withColumn(output_col, trim(col(\"_step4_spaces\")))\n",
    "        .drop(\"_step1_trim\", \"_step2_lower\", \"_step3_alphanum\", \"_step4_spaces\")\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "df_reviews_cleaned = clean_text_pipeline(df_reviews, \"review\", \"review_cleaned\")\n",
    "print(\"Cleaned reviews:\")\n",
    "df_reviews_cleaned.select(\"review\", \"review_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce572f9e",
   "metadata": {},
   "source": [
    "### Practical Exercise: Product Review Cleaning\n",
    "Tasks:\n",
    "- Clean the review text (remove special chars, normalize case, trim)\n",
    "- Extract the word count from each cleaned review\n",
    "- Identify reviews that mention negative keywords (return, refund, broken)\n",
    "- Calculate the average rating for reviews with negative keywords vs others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10beee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------+------+\n",
      "|id |review                                                    |rating|\n",
      "+---+----------------------------------------------------------+------+\n",
      "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |\n",
      "|2  |Product broke after 1 week. Requesting refund immediately.|1     |\n",
      "|3  |Decent product for the price. Nothing special but works.  |3     |\n",
      "|4  |DO NOT BUY! Cheap quality, had to return it!              |1     |\n",
      "|5  |Excellent! Better than expected. 100% recommend.          |5     |\n",
      "|6  |Item was damaged during shipping. Broken on arrival.      |1     |\n",
      "|7  |Good value for money. Satisfied with purchase.            |4     |\n",
      "|8  |WORST PURCHASE EVER! Want my money back!!!                |1     |\n",
      "|9  |Nice product, works as described. Happy customer.         |4     |\n",
      "|10 |Meh, it's okay. Nothing to complain about.                |3     |\n",
      "+---+----------------------------------------------------------+------+\n",
      "\n",
      "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+--------------------+\n",
      "|id |review                                                    |rating|clean_review                                            |word_count|has_negative_keyword|\n",
      "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+--------------------+\n",
      "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |0                   |\n",
      "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |1                   |\n",
      "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |9         |0                   |\n",
      "|4  |DO NOT BUY! Cheap quality, had to return it!              |1     |do not buy cheap quality had to return it               |9         |1                   |\n",
      "|5  |Excellent! Better than expected. 100% recommend.          |5     |excellent better than expected 100 recommend            |6         |0                   |\n",
      "|6  |Item was damaged during shipping. Broken on arrival.      |1     |item was damaged during shipping broken on arrival      |8         |1                   |\n",
      "|7  |Good value for money. Satisfied with purchase.            |4     |good value for money satisfied with purchase            |7         |0                   |\n",
      "|8  |WORST PURCHASE EVER! Want my money back!!!                |1     |worst purchase ever want my money back                  |7         |0                   |\n",
      "|9  |Nice product, works as described. Happy customer.         |4     |nice product works as described happy customer          |7         |0                   |\n",
      "|10 |Meh, it's okay. Nothing to complain about.                |3     |meh its okay nothing to complain about                  |7         |0                   |\n",
      "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+--------------------+\n",
      "\n",
      "+--------------------+------------------+\n",
      "|has_negative_keyword|    average_rating|\n",
      "+--------------------+------------------+\n",
      "|                   0|3.5714285714285716|\n",
      "|                   1|               1.0|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extended reviews dataset\n",
    "extended_reviews = [\n",
    "    (1, \"LOVE IT! Fast shipping, great quality. Will order more!\", 5),\n",
    "    (2, \"Product broke after 1 week. Requesting refund immediately.\", 1),\n",
    "    (3, \"Decent product for the price. Nothing special but works.\", 3),\n",
    "    (4, \"DO NOT BUY! Cheap quality, had to return it!\", 1),\n",
    "    (5, \"Excellent! Better than expected. 100% recommend.\", 5),\n",
    "    (6, \"Item was damaged during shipping. Broken on arrival.\", 1),\n",
    "    (7, \"Good value for money. Satisfied with purchase.\", 4),\n",
    "    (8, \"WORST PURCHASE EVER! Want my money back!!!\", 1),\n",
    "    (9, \"Nice product, works as described. Happy customer.\", 4),\n",
    "    (10, \"Meh, it's okay. Nothing to complain about.\", 3),\n",
    "]\n",
    "\n",
    "df_extended = spark.createDataFrame(extended_reviews, [\"id\", \"review\", \"rating\"])\n",
    "df_extended.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lower,\n",
    "    trim,\n",
    "    regexp_replace,\n",
    "    split,\n",
    "    size,\n",
    "    when,\n",
    "    avg,\n",
    ")\n",
    "\n",
    "# Clean the extended reviews\n",
    "df_processed = df_extended.withColumn(\n",
    "    \"clean_review\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                lower(col(\"review\")),  \n",
    "                \"[^a-z0-9\\\\s]\",        \n",
    "                \"\"\n",
    "            ),\n",
    "            \"\\\\s+\",                    \n",
    "            \" \"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Word Count\n",
    "df_processed = df_processed.withColumn(\n",
    "    \"word_count\", size(split(col(\"clean_review\"), \" \"))\n",
    ")\n",
    "\n",
    "# Negative keywords flag\n",
    "df_processed = df_processed.withColumn(\n",
    "    \"has_negative_keyword\",\n",
    "    when(col(\"clean_review\").rlike(\"return|refund|broken\"), 1).otherwise(0),\n",
    ")\n",
    "\n",
    "# Processed DataFrame\n",
    "df_processed.show(truncate=False)\n",
    "\n",
    "# Average Rating Comparision\n",
    "df_processed.groupBy(\"has_negative_keyword\").agg(\n",
    "    avg(\"rating\").alias(\"average_rating\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060290e",
   "metadata": {},
   "source": [
    "## Regular Expressions in Spark\n",
    "\n",
    "### Regex in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact data:\n",
      "+---+-----------+----------------------+-------------------+----------------------------------+\n",
      "|id |name       |email                 |phone              |address                           |\n",
      "+---+-----------+----------------------+-------------------+----------------------------------+\n",
      "|1  |John Smith |john.smith@email.com  |(555) 123-4567     |123 Main St, New York, NY 10001   |\n",
      "|2  |Jane Doe   |jane_doe@company.co.uk|555-987-6543       |456 Oak Ave, Los Angeles, CA 90001|\n",
      "|3  |Bob Wilson |bob123@test.org       |5551234567         |789 Pine Rd, Chicago, IL 60601    |\n",
      "|4  |Alice Brown|alice.brown@domain.net|(555)456-7890      |321 Elm Blvd, Houston, TX 77001   |\n",
      "|5  |Charlie Lee|invalid-email         |phone: 555-111-2222|PO Box 100, Phoenix, AZ 85001     |\n",
      "+---+-----------+----------------------+-------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Data\n",
    "from nt import truncate\n",
    "\n",
    "\n",
    "contact_data = [\n",
    "    (\n",
    "        1,\n",
    "        \"John Smith\",\n",
    "        \"john.smith@email.com\",\n",
    "        \"(555) 123-4567\",\n",
    "        \"123 Main St, New York, NY 10001\",\n",
    "    ),\n",
    "    (\n",
    "        2,\n",
    "        \"Jane Doe\",\n",
    "        \"jane_doe@company.co.uk\",\n",
    "        \"555-987-6543\",\n",
    "        \"456 Oak Ave, Los Angeles, CA 90001\",\n",
    "    ),\n",
    "    (\n",
    "        3,\n",
    "        \"Bob Wilson\",\n",
    "        \"bob123@test.org\",\n",
    "        \"5551234567\",\n",
    "        \"789 Pine Rd, Chicago, IL 60601\",\n",
    "    ),\n",
    "    (\n",
    "        4,\n",
    "        \"Alice Brown\",\n",
    "        \"alice.brown@domain.net\",\n",
    "        \"(555)456-7890\",\n",
    "        \"321 Elm Blvd, Houston, TX 77001\",\n",
    "    ),\n",
    "    (\n",
    "        5,\n",
    "        \"Charlie Lee\",\n",
    "        \"invalid-email\",\n",
    "        \"phone: 555-111-2222\",\n",
    "        \"PO Box 100, Phoenix, AZ 85001\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_contacts = spark.createDataFrame(\n",
    "    contact_data, [\"id\", \"name\", \"email\", \"phone\", \"address\"]\n",
    ")\n",
    "\n",
    "print(\"Contact data:\")\n",
    "df_contacts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638677c",
   "metadata": {},
   "source": [
    "### Pattern Matching with rlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email validation:\n",
      "+-----------+----------------------+--------------+\n",
      "|name       |email                 |is_valid_email|\n",
      "+-----------+----------------------+--------------+\n",
      "|John Smith |john.smith@email.com  |true          |\n",
      "|Jane Doe   |jane_doe@company.co.uk|false         |\n",
      "|Bob Wilson |bob123@test.org       |true          |\n",
      "|Alice Brown|alice.brown@domain.net|true          |\n",
      "|Charlie Lee|invalid-email         |false         |\n",
      "+-----------+----------------------+--------------+\n",
      "\n",
      "Company domain emails:\n",
      "+--------+--------------------+\n",
      "|    name|               email|\n",
      "+--------+--------------------+\n",
      "|Jane Doe|jane_doe@company....|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check valid email format\n",
    "email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-Z]{2,}$\"\n",
    "\n",
    "df_email_check = df_contacts.withColumn(\n",
    "    \"is_valid_email\", col(\"email\").rlike(email_pattern)\n",
    ")\n",
    "\n",
    "print(\"Email validation:\")\n",
    "df_email_check.select(\"name\", \"email\", \"is_valid_email\").show(truncate=False)\n",
    "\n",
    "# Filter for specific domain emails\n",
    "df_company_emails = df_contacts.filter(col(\"email\").rlike(r\"@company\\.\"))\n",
    "\n",
    "print(\"Company domain emails:\")\n",
    "df_company_emails.select(\"name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa764565",
   "metadata": {},
   "source": [
    "### Extracting Data with regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d69b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted email components:\n",
      "+----------------------+--------------+------------+---------+\n",
      "|email                 |email_username|email_domain|email_tld|\n",
      "+----------------------+--------------+------------+---------+\n",
      "|john.smith@email.com  |john.smith    |email       |com      |\n",
      "|jane_doe@company.co.uk|jane_doe      |company     |co.uk    |\n",
      "|bob123@test.org       |bob123        |test        |org      |\n",
      "|alice.brown@domain.net|alice.brown   |domain      |net      |\n",
      "|invalid-email         |              |            |         |\n",
      "+----------------------+--------------+------------+---------+\n",
      "\n",
      "Phone number extraction adn normalisation:\n",
      "+-------------------+------------+---------+----------------+\n",
      "|phone              |phone_digits|area_code|phone_normalised|\n",
      "+-------------------+------------+---------+----------------+\n",
      "|(555) 123-4567     |5551234567  |555      |(555)123-4567   |\n",
      "|555-987-6543       |5559876543  |555      |(555)987-6543   |\n",
      "|5551234567         |5551234567  |555      |(555)123-4567   |\n",
      "|(555)456-7890      |5554567890  |555      |(555)456-7890   |\n",
      "|phone: 555-111-2222|5551112222  |555      |(555)111-2222   |\n",
      "+-------------------+------------+---------+----------------+\n",
      "\n",
      "Extracted address components:\n",
      "+----------------------------------+------------------------+-----------+-----+--------+\n",
      "|address                           |street                  |city       |state|zip_code|\n",
      "+----------------------------------+------------------------+-----------+-----+--------+\n",
      "|123 Main St, New York, NY 10001   |123 Main St, New York   |New York   |NY   |10001   |\n",
      "|456 Oak Ave, Los Angeles, CA 90001|456 Oak Ave, Los Angeles|Los Angeles|CA   |90001   |\n",
      "|789 Pine Rd, Chicago, IL 60601    |789 Pine Rd, Chicago    |Chicago    |IL   |60601   |\n",
      "|321 Elm Blvd, Houston, TX 77001   |321 Elm Blvd, Houston   |Houston    |TX   |77001   |\n",
      "|PO Box 100, Phoenix, AZ 85001     |PO Box 100, Phoenix     |Phoenix    |AZ   |85001   |\n",
      "+----------------------------------+------------------------+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract email components\n",
    "df_email_parts = (\n",
    "    df_contacts.withColumn(\n",
    "        \"email_username\", regexp_extract(col(\"email\"), r\"^([\\w.+-]+)@\", 1)\n",
    "    )\n",
    "    .withColumn(\"email_domain\", regexp_extract(col(\"email\"), r\"@([\\w-]+)\\.\", 1))\n",
    "    .withColumn(\"email_tld\", regexp_extract(col(\"email\"), r\"\\.([a-zA-Z.]+)$\", 1))\n",
    ")\n",
    "\n",
    "print(\"Extracted email components:\")\n",
    "df_email_parts.select(\"email\", \"email_username\", \"email_domain\", \"email_tld\").show(\n",
    "    truncate=False\n",
    ")\n",
    "\n",
    "# Extract and normalise phone numbers\n",
    "df_phone_extract = (\n",
    "    df_contacts.withColumn(\"phone_digits\", regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"))\n",
    "    .withColumn(\n",
    "        \"area_code\",\n",
    "        regexp_extract(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), r\"^(\\d{3})\", 1),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"phone_normalised\",\n",
    "        concat(\n",
    "            lit(\"(\"),\n",
    "            substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 1, 3),\n",
    "            lit(\")\"),\n",
    "            substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 4, 3),\n",
    "            lit(\"-\"),\n",
    "            substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 7, 4),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Phone number extraction and normalisation:\")\n",
    "df_phone_extract.select(\"phone\", \"phone_digits\", \"area_code\", \"phone_normalised\").show(\n",
    "    truncate=False\n",
    ")\n",
    "\n",
    "# Extract address components\n",
    "df_address_parts = (\n",
    "    df_contacts.withColumn(\"street\", regexp_extract(col(\"address\"), r\"^(.+),\", 1))\n",
    "    .withColumn(\"city\", regexp_extract(col(\"address\"), r\", ([^,]+), [A-Z]{2}\", 1))\n",
    "    .withColumn(\"state\", regexp_extract(col(\"address\"), r\", ([A-Z]{2}) \\d\", 1))\n",
    "    .withColumn(\"zip_code\", regexp_extract(col(\"address\"), r\"(\\d{5})$\", 1))\n",
    ")\n",
    "\n",
    "print(\"Extracted address components:\")\n",
    "df_address_parts.select(\"address\", \"street\", \"city\", \"state\", \"zip_code\").show(\n",
    "    truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcd6e0",
   "metadata": {},
   "source": [
    "### Replacing Patterns with regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64494e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked sensitive data:\n",
      "+-----------+----------------------+-------------------+-------------------+--------------+\n",
      "|name       |email                 |email_masked       |phone              |phone_masked  |\n",
      "+-----------+----------------------+-------------------+-------------------+--------------+\n",
      "|John Smith |john.smith@email.com  |jo***@email.com    |(555) 123-4567     |(***) ***-4567|\n",
      "|Jane Doe   |jane_doe@company.co.uk|ja***@company.co.uk|555-987-6543       |(***) ***-6543|\n",
      "|Bob Wilson |bob123@test.org       |bo***@test.org     |5551234567         |(***) ***-4567|\n",
      "|Alice Brown|alice.brown@domain.net|al***@domain.net   |(555)456-7890      |(***) ***-7890|\n",
      "|Charlie Lee|invalid-email         |invalid-email      |phone: 555-111-2222|(***) ***-2222|\n",
      "+-----------+----------------------+-------------------+-------------------+--------------+\n",
      "\n",
      "Text standardization:\n",
      "+---+---------------------------+---------------------------+\n",
      "|id |text                       |text_clean                 |\n",
      "+---+---------------------------+---------------------------+\n",
      "|1  |Hello World!!!             |hello world!!!             |\n",
      "|2  |Multiple spaces here       |multiple spaces here       |\n",
      "|3  |Lots of!!!!! punctuation???|lots of!!!!! punctuation???|\n",
      "|4  |   Leading and trailing    |leading and trailing       |\n",
      "|5  |MixED CaSe TeXt            |mixed case text            |\n",
      "+---+---------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mask sensitive data\n",
    "df_masked = df_contacts.withColumn(\n",
    "    \"email_masked\", regexp_replace(col(\"email\"), r\"^(.{2}).*(@.*)$\", r\"$1***$2\")\n",
    ").withColumn(\n",
    "    \"phone_masked\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"),\n",
    "        r\"^(\\d{3})(\\d{3})(\\d{4})$\",\n",
    "        r\"(***) ***-$3\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Masked sensitive data:\")\n",
    "df_masked.select(\"name\", \"email\", \"email_masked\", \"phone\", \"phone_masked\").show(\n",
    "    truncate=False\n",
    ")\n",
    "\n",
    "# Clean and standarise data\n",
    "messy_text_data = [\n",
    "    (1, \"Hello World!!!\"),\n",
    "    (2, \"Multiple spaces here\"),\n",
    "    (3, \"Lots of!!!!! punctuation???\"),\n",
    "    (4, \"   Leading and trailing   \"),\n",
    "    (5, \"MixED CaSe TeXt\"),\n",
    "]\n",
    "\n",
    "df_messy_text = spark.createDataFrame(messy_text_data, [\"id\", \"text\"])\n",
    "\n",
    "df_standardized = df_messy_text.withColumn(\n",
    "    \"text_clean\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(lower(col(\"text\")), rf\"([!?.]){(2,)}\", r\"$1\"), r\"\\s+\", \" \"\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Text standardization:\")\n",
    "df_standardized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93ba5e",
   "metadata": {},
   "source": [
    "### Practical Exercise: Log File Parsing\n",
    "\n",
    "Parse web server access logs using regex to extract structured information.\n",
    "- Extract IP address, timestamp, HTTP method, URL path, status code, and response size\n",
    "- Filter for error status codes (4xx and 5xx)\n",
    "- Count requests by HTTP method\n",
    "- Identify the most accessed URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b7a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample access logs:\n",
      "+---+------------------------------------------------------------------------------------+\n",
      "|id |log_line                                                                            |\n",
      "+---+------------------------------------------------------------------------------------+\n",
      "|1  |192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] \"GET /index.html HTTP/1.1\" 200 1234  |\n",
      "|2  |10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567       |\n",
      "|3  |192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890   |\n",
      "|4  |172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234   |\n",
      "|5  |192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100     |\n",
      "|6  |10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456      |\n",
      "|7  |192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0 |\n",
      "|8  |172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234    |\n",
      "|9  |10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89        |\n",
      "|10 |192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678|\n",
      "+---+------------------------------------------------------------------------------------+\n",
      "\n",
      "Parsed strctured logs:\n",
      "+-------------+--------------------------+-----------+-------------+-----------+-------------+\n",
      "|ip_address   |timestamp                 |http_method|url_path     |status_code|response_size|\n",
      "+-------------+--------------------------+-----------+-------------+-----------+-------------+\n",
      "|192.168.1.100|15/Jan/2024:10:30:45 +0000|GET        |/index.html  |200        |1234         |\n",
      "|10.0.0.50    |15/Jan/2024:10:31:00 +0000|POST       |/api/users   |201        |567          |\n",
      "|192.168.1.101|15/Jan/2024:10:31:15 +0000|GET        |/about.html  |200        |890          |\n",
      "|172.16.0.25  |15/Jan/2024:10:31:30 +0000|GET        |/missing.html|404        |234          |\n",
      "|192.168.1.100|15/Jan/2024:10:31:45 +0000|GET        |/api/data    |500        |100          |\n",
      "|10.0.0.75    |15/Jan/2024:10:32:00 +0000|PUT        |/api/users/1 |200        |456          |\n",
      "|192.168.1.102|15/Jan/2024:10:32:15 +0000|DELETE     |/api/users/2 |204        |0            |\n",
      "|172.16.0.30  |15/Jan/2024:10:32:30 +0000|GET        |/index.html  |200        |1234         |\n",
      "|10.0.0.50    |15/Jan/2024:10:32:45 +0000|POST       |/api/login   |401        |89           |\n",
      "|192.168.1.100|15/Jan/2024:10:33:00 +0000|GET        |/api/products|200        |5678         |\n",
      "+-------------+--------------------------+-----------+-------------+-----------+-------------+\n",
      "\n",
      "Error requests (4xx and 5xx):\n",
      "+-------------+-------------+-----------+\n",
      "|   ip_address|     url_path|status_code|\n",
      "+-------------+-------------+-----------+\n",
      "|  172.16.0.25|/missing.html|        404|\n",
      "|192.168.1.100|    /api/data|        500|\n",
      "|    10.0.0.50|   /api/login|        401|\n",
      "+-------------+-------------+-----------+\n",
      "\n",
      "Request count by HTTP method:\n",
      "+-----------+-------------+\n",
      "|http_method|request_count|\n",
      "+-----------+-------------+\n",
      "|        GET|            6|\n",
      "|       POST|            2|\n",
      "|        PUT|            1|\n",
      "|     DELETE|            1|\n",
      "+-----------+-------------+\n",
      "\n",
      "Most accessed URLs:\n",
      "+-------------+------------+\n",
      "|     url_path|access_count|\n",
      "+-------------+------------+\n",
      "|  /index.html|           2|\n",
      "|   /api/users|           1|\n",
      "|  /about.html|           1|\n",
      "|/missing.html|           1|\n",
      "|    /api/data|           1|\n",
      "| /api/users/1|           1|\n",
      "| /api/users/2|           1|\n",
      "|   /api/login|           1|\n",
      "|/api/products|           1|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Apache access logs\n",
    "log_data = [\n",
    "    (\n",
    "        1,\n",
    "        '192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] \"GET /index.html HTTP/1.1\" 200 1234',\n",
    "    ),\n",
    "    (\n",
    "        2,\n",
    "        '10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567',\n",
    "    ),\n",
    "    (\n",
    "        3,\n",
    "        '192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890',\n",
    "    ),\n",
    "    (\n",
    "        4,\n",
    "        '172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234',\n",
    "    ),\n",
    "    (\n",
    "        5,\n",
    "        '192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100',\n",
    "    ),\n",
    "    (\n",
    "        6,\n",
    "        '10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456',\n",
    "    ),\n",
    "    (\n",
    "        7,\n",
    "        '192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0',\n",
    "    ),\n",
    "    (\n",
    "        8,\n",
    "        '172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234',\n",
    "    ),\n",
    "    (9, '10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89'),\n",
    "    (\n",
    "        10,\n",
    "        '192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678',\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_logs = spark.createDataFrame(log_data, [\"id\", \"log_line\"])\n",
    "print(\"sample access logs:\")\n",
    "df_logs.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_extract, count\n",
    "\n",
    "# Extract components from Apache log format\n",
    "df_parsed = (\n",
    "    df_logs.withColumn(\"ip_address\", regexp_extract(col(\"log_line\"), r\"^([\\d\\.]+)\", 1))\n",
    "    .withColumn(\"timestamp\", regexp_extract(col(\"log_line\"), r\"\\[([^\\]]+)\\]\", 1))\n",
    "    .withColumn(\n",
    "        \"http_method\", regexp_extract(col(\"log_line\"), r\"\\\"(GET|POST|PUT|DELETE)\", 1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"url_path\",\n",
    "        regexp_extract(col(\"log_line\"), r\"\\\"(?:GET|POST|PUT|DELETE) ([^ ]+)\", 1),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"status_code\", regexp_extract(col(\"log_line\"), r\"\\\" (\\d{3})\", 1).cast(\"int\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"response_size\", regexp_extract(col(\"log_line\"), r\" (\\d+)$\", 1).cast(\"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Parsed strctured logs:\")\n",
    "\n",
    "df_parsed.select(\n",
    "    \"ip_address\", \"timestamp\", \"http_method\", \"url_path\", \"status_code\", \"response_size\"\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "# Filter Error Codes\n",
    "df_errors = df_parsed.filter(col(\"status_code\") >= 400)\n",
    "\n",
    "print(\"Error requests (4xx and 5xx):\")\n",
    "df_errors.select(\"ip_address\", \"url_path\", \"status_code\").show()\n",
    "\n",
    "\n",
    "# Count Requests by HTTP Method\n",
    "print(\"Request count by HTTP method:\")\n",
    "df_parsed.groupBy(\"http_method\").agg(count(\"*\").alias(\"request_count\")).show()\n",
    "\n",
    "\n",
    "# Most accessed URLs\n",
    "print(\"Most accessed URLs:\")\n",
    "df_parsed.groupBy(\"url_path\").agg(count(\"*\").alias(\"access_count\")).orderBy(\n",
    "    col(\"access_count\").desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8749b7",
   "metadata": {},
   "source": [
    "## Building Complete Preprocessing Pipelines\n",
    "\n",
    "### Combining Multiple Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca62830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw messy data:\n",
      "+---+------------+----------------------+-------------------+-----+----------+--------------+\n",
      "|id |name        |email                 |phone              |age  |salary    |hire_date     |\n",
      "+---+------------+----------------------+-------------------+-----+----------+--------------+\n",
      "|1  | JOHN SMITH |john.smith@EMAIL.com  |(555) 123-4567     |25   |$50,000.00|2024-01-15    |\n",
      "|2  |jane doe    |JANE_DOE@COMPANY.CO.UK|555-987-6543       |N/A  |60000     |15/02/2024    |\n",
      "|3  |Bob Wilson  |invalid-email         |5551234567         |35   |75,000    |March 20, 2024|\n",
      "|4  |NULL        |alice@domain.net      |(555)456-7890      |28   |NULL      |2024-04-10    |\n",
      "|5  |Charlie Lee |                      |phone: 555-111-2222|forty|$80,000   |NULL          |\n",
      "+---+------------+----------------------+-------------------+-----+----------+--------------+\n",
      "\n",
      "Cleaned data:\n",
      "+---+-----------+--------------------+-------------+----+-------+----------+\n",
      "|id |name       |email               |phone        |age |salary |hire_date |\n",
      "+---+-----------+--------------------+-------------+----+-------+----------+\n",
      "|1  |John Smith |john.smith@email.com|(555)123-4567|25  |50000.0|2024-01-15|\n",
      "|2  |Jane Doe   |NULL                |(555)987-6543|NULL|60000.0|2024-02-15|\n",
      "|3  |Bob Wilson |NULL                |(555)123-4567|35  |75000.0|2024-03-20|\n",
      "|4  |Employee_4 |alice@domain.net    |(555)456-7890|28  |NULL   |2024-04-10|\n",
      "|5  |Charlie Lee|NULL                |(555)111-2222|NULL|80000.0|NULL      |\n",
      "+---+-----------+--------------------+-------------+----+-------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Real world messy dataset\n",
    "raw_data = [\n",
    "    (\n",
    "        \"1\",\n",
    "        \" JOHN SMITH \",\n",
    "        \"john.smith@EMAIL.com\",\n",
    "        \"(555) 123-4567\",\n",
    "        \"25\",\n",
    "        \"$50,000.00\",\n",
    "        \"2024-01-15\",\n",
    "    ),\n",
    "    (\n",
    "        \"2\",\n",
    "        \"jane doe\",\n",
    "        \"JANE_DOE@COMPANY.CO.UK\",\n",
    "        \"555-987-6543\",\n",
    "        \"N/A\",\n",
    "        \"60000\",\n",
    "        \"15/02/2024\",\n",
    "    ),\n",
    "    (\n",
    "        \"3\",\n",
    "        \"Bob Wilson\",\n",
    "        \"invalid-email\",\n",
    "        \"5551234567\",\n",
    "        \"35\",\n",
    "        \"75,000\",\n",
    "        \"March 20, 2024\",\n",
    "    ),\n",
    "    (\"4\", None, \"alice@domain.net\", \"(555)456-7890\", \"28\", None, \"2024-04-10\"),\n",
    "    (\"5\", \"Charlie Lee\", \"\", \"phone: 555-111-2222\", \"forty\", \"$80,000\", None),\n",
    "]\n",
    "\n",
    "df_raw = spark.createDataFrame(\n",
    "    raw_data, [\"id\", \"name\", \"email\", \"phone\", \"age\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "print(\"Raw messy data:\")\n",
    "df_raw.show(truncate=False)\n",
    "\n",
    "\n",
    "def preprocess_employee_data(df):\n",
    "    # Clean and standarise data\n",
    "    df = df.withColumn(\n",
    "        \"name_clean\",\n",
    "        when(\n",
    "            col(\"name\").isNull() | (trim(col(\"name\")) == \"\"),\n",
    "            concat(lit(\"Employee_\"), col(\"id\")),\n",
    "        ).otherwise(expr(\"initcap(trim(name))\")),\n",
    "    )\n",
    "\n",
    "    # Clean and Validate Email\n",
    "    email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-z]{2,}$\"\n",
    "    df = df.withColumn(\n",
    "        \"email_clean\",\n",
    "        when(\n",
    "            lower(trim(col(\"email\"))).rlike(email_pattern), lower(trim(col(\"email\")))\n",
    "        ).otherwise(None),\n",
    "    )\n",
    "\n",
    "    # Normalise Phone Number\n",
    "    df = df.withColumn(\n",
    "        \"phone_digits\", regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\")\n",
    "    ).withColumn(\n",
    "        \"phone_clean\",\n",
    "        when(\n",
    "            length(col(\"phone_digits\")) == 10,\n",
    "            concat(\n",
    "                lit(\"(\"),\n",
    "                substring(col(\"phone_digits\"), 1, 3),\n",
    "                lit(\")\"),\n",
    "                substring(col(\"phone_digits\"), 4, 3),\n",
    "                lit(\"-\"),\n",
    "                substring(col(\"phone_digits\"), 7, 4),\n",
    "            ),\n",
    "        ).otherwise(None),\n",
    "    )\n",
    "\n",
    "    # Convert Age to Integer\n",
    "    df = df.withColumn(\"age_clean\", col(\"age\").cast(IntegerType()))\n",
    "\n",
    "    # Clean and Convert Salary\n",
    "    df = df.withColumn(\n",
    "        \"salary_clean\", regexp_replace(col(\"salary\"), r\"[$,]\", \"\").cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    # Parse hire date\n",
    "    df = df.withColumn(\n",
    "        \"hire_date_clean\",\n",
    "        coalesce(\n",
    "            to_date(col(\"hire_date\"), \"yyyy-MM-dd\"),\n",
    "            to_date(col(\"hire_date\"), \"dd/MM/yyyy\"),\n",
    "            to_date(col(\"hire_date\"), \"MMMM dd, yyyy\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        col(\"id\").cast(IntegerType()).alias(\"id\"),\n",
    "        col(\"name_clean\").alias(\"name\"),\n",
    "        col(\"email_clean\").alias(\"email\"),\n",
    "        col(\"phone_clean\").alias(\"phone\"),\n",
    "        col(\"age_clean\").alias(\"age\"),\n",
    "        col(\"salary_clean\").alias(\"salary\"),\n",
    "        col(\"hire_date_clean\").alias(\"hire_date\"),\n",
    "    )\n",
    "\n",
    "\n",
    "df_clean = preprocess_employee_data(df_raw)\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "df_clean.show(truncate=False)\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f0872",
   "metadata": {},
   "source": [
    "### Data Quality Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a5d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "\n",
      "Total rows: 5\n",
      "\n",
      "--- Missing Value Summary (Cleaned Data) ---\n",
      " id: 0 nulls (0.0%) [OK]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " name: 0 nulls (0.0%) [OK]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " email: 3 nulls (60.0%) [CRITICAL]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " phone: 0 nulls (0.0%) [OK]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " age: 2 nulls (40.0%) [CRITICAL]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " salary: 1 nulls (20.0%) [WARNING]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n",
      " hire_date: 1 nulls (20.0%) [WARNING]\n",
      "\n",
      "--- Overall Data Quality Score ---\n",
      " Completeness: 80.0%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_quality_report(df_original, df_cleaned):\n",
    "    total_rows = df_original.count()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal rows: {total_rows}\")\n",
    "    print(\"\\n--- Missing Value Summary (Cleaned Data) ---\")\n",
    "\n",
    "    for col_name in df_cleaned.columns:\n",
    "        null_count = df_cleaned.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100\n",
    "        status = \"OK\" if null_pct < 10 else \"WARNING\" if null_pct < 30 else \"CRITICAL\"\n",
    "        print(f\" {col_name}: {null_count} nulls ({null_pct:.1f}%) [{status}]\")\n",
    "        total_cells = total_rows * len(df_cleaned.columns)\n",
    "        null_cells = sum(\n",
    "            df_cleaned.filter(col(c).isNull()).count() for c in df_cleaned.columns\n",
    "        )\n",
    "        completeness = ((total_cells - null_cells) / total_cells) * 100\n",
    "        print(f\"\\n--- Overall Data Quality Score ---\")\n",
    "        print(f\" Completeness: {completeness:.1f}%\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "generate_quality_report(df_raw, df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e84aef",
   "metadata": {},
   "source": [
    "## Final Challenge: End-to-End Data Cleaning\n",
    "- Identify and report all data quality issues\n",
    "- Handle missing values appropriately for each column\n",
    "- Standardize text fields (product names, categories)\n",
    "- Parse and validate dates\n",
    "- Clean and validate email addresses\n",
    "- Extract useful information using regex\n",
    "- Create a comprehensive data quality report\n",
    "- Output the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw e-commerce data:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+-----------+------------------+-------+--------+-------------------+----------+\n",
      "|order_id|product                |category   |email             |price  |quantity|order_date         |status    |\n",
      "+--------+-----------------------+-----------+------------------+-------+--------+-------------------+----------+\n",
      "|ORD001  | LAPTOP Computer       |ELECTRONICS|john@email.com    |$999.99|2       |2024-01-15 10:30:00|Completed |\n",
      "|ORD002  |wireless mouse         |electronics|JANE@COMPANY.CO.UK|29.99  |NULL    |15/01/2024         |completed |\n",
      "|ORD003  |USB-C Cable (6ft)      |Accessories|invalid-email     |$12.50 |5       |January 16, 2024   |PENDING   |\n",
      "|ORD004  |NULL                   |CLOTHING   |bob@test.org      |N/A    |1       |2024-01-16         |Cancelled |\n",
      "|ORD005  |Running Shoes - Size 10| clothing  |                  |89.00  |2       |NULL               |Shipped   |\n",
      "|ORD006  |HDMI Cable!!!          |accessories|alice@domain.net  |$15.00 |10      |2024-01-17 14:20:00|completed |\n",
      "|ORD007  |smartphone case        |Electronics|charlie@email.com |$25    |3       |17-Jan-2024        |Processing|\n",
      "|ORD008  | Winter Jacket         |Clothing   |N/A               |$149.99|NULL    |2024-01-18         |SHIPPED   |\n",
      "+--------+-----------------------+-----------+------------------+-------+--------+-------------------+----------+\n",
      "\n",
      "Data Quality Report\n",
      "+-------------+---------------+-------------+-------------+----------------+------------+\n",
      "|total_records|missing_product|invalid_email|invalid_price|invalid_quantity|invalid_date|\n",
      "+-------------+---------------+-------------+-------------+----------------+------------+\n",
      "|            8|              1|            3|            1|               2|           4|\n",
      "+-------------+---------------+-------------+-------------+----------------+------------+\n",
      "\n",
      "Final Cleaned Dataset:\n",
      "+--------+----------------------+-----------+------------------+------+--------+-------------------+----------+--------------+\n",
      "|order_id|product               |category   |email             |price |quantity|order_date         |status    |size_extracted|\n",
      "+--------+----------------------+-----------+------------------+------+--------+-------------------+----------+--------------+\n",
      "|ORD001  |Laptop Computer       |Electronics|john@email.com    |999.99|2       |2024-01-15 10:30:00|Completed |              |\n",
      "|ORD002  |Wireless Mouse        |Electronics|jane@company.co.uk|29.99 |NULL    |2024-01-15 00:00:00|Completed |              |\n",
      "|ORD003  |Usbc Cable 6ft        |Accessories|NULL              |12.5  |5       |2024-01-16 00:00:00|Pending   |              |\n",
      "|ORD004  |NULL                  |Clothing   |bob@test.org      |NULL  |1       |NULL               |Cancelled |NULL          |\n",
      "|ORD005  |Running Shoes  Size 10|Clothing   |NULL              |89.0  |2       |NULL               |Shipped   |10            |\n",
      "|ORD006  |Hdmi Cable            |Accessories|alice@domain.net  |15.0  |10      |2024-01-17 14:20:00|Completed |              |\n",
      "|ORD007  |Smartphone Case       |Electronics|charlie@email.com |25.0  |3       |NULL               |Processing|              |\n",
      "|ORD008  |Winter Jacket         |Clothing   |NULL              |149.99|NULL    |NULL               |Shipped   |              |\n",
      "+--------+----------------------+-----------+------------------+------+--------+-------------------+----------+--------------+\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- size_extracted: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E-commerce Dataset\n",
    "ecommerce_data = [\n",
    "    (\n",
    "        \"ORD001\",\n",
    "        \" LAPTOP Computer \",\n",
    "        \"ELECTRONICS\",\n",
    "        \"john@email.com\",\n",
    "        \"$999.99\",\n",
    "        \"2\",\n",
    "        \"2024-01-15 10:30:00\",\n",
    "        \"Completed\",\n",
    "    ),\n",
    "    (\n",
    "        \"ORD002\",\n",
    "        \"wireless mouse\",\n",
    "        \"electronics\",\n",
    "        \"JANE@COMPANY.CO.UK\",\n",
    "        \"29.99\",\n",
    "        None,\n",
    "        \"15/01/2024\",\n",
    "        \"completed\",\n",
    "    ),\n",
    "    (\n",
    "        \"ORD003\",\n",
    "        \"USB-C Cable (6ft)\",\n",
    "        \"Accessories\",\n",
    "        \"invalid-email\",\n",
    "        \"$12.50\",\n",
    "        \"5\",\n",
    "        \"January 16, 2024\",\n",
    "        \"PENDING\",\n",
    "    ),\n",
    "    (\"ORD004\", None, \"CLOTHING\", \"bob@test.org\", \"N/A\", \"1\", \"2024-01-16\", \"Cancelled\"),\n",
    "    (\n",
    "        \"ORD005\",\n",
    "        \"Running Shoes - Size 10\",\n",
    "        \" clothing \",\n",
    "        \"\",\n",
    "        \"89.00\",\n",
    "        \"2\",\n",
    "        None,\n",
    "        \"Shipped\",\n",
    "    ),\n",
    "    (\n",
    "        \"ORD006\",\n",
    "        \"HDMI Cable!!!\",\n",
    "        \"accessories\",\n",
    "        \"alice@domain.net\",\n",
    "        \"$15.00\",\n",
    "        \"10\",\n",
    "        \"2024-01-17 14:20:00\",\n",
    "        \"completed\",\n",
    "    ),\n",
    "    (\n",
    "        \"ORD007\",\n",
    "        \"smartphone case\",\n",
    "        \"Electronics\",\n",
    "        \"charlie@email.com\",\n",
    "        \"$25\",\n",
    "        \"3\",\n",
    "        \"17-Jan-2024\",\n",
    "        \"Processing\",\n",
    "    ),\n",
    "    (\n",
    "        \"ORD008\",\n",
    "        \" Winter Jacket \",\n",
    "        \"Clothing\",\n",
    "        \"N/A\",\n",
    "        \"$149.99\",\n",
    "        None,\n",
    "        \"2024-01-18\",\n",
    "        \"SHIPPED\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_ecommerce = spark.createDataFrame(\n",
    "    ecommerce_data,\n",
    "    [\n",
    "        \"order_id\",\n",
    "        \"product\",\n",
    "        \"category\",\n",
    "        \"email\",\n",
    "        \"price\",\n",
    "        \"quantity\",\n",
    "        \"order_date\",\n",
    "        \"status\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Raw e-commerce data:\")\n",
    "df_ecommerce.show(truncate=False)\n",
    "\n",
    "# Imports\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    trim,\n",
    "    lower,\n",
    "    upper,\n",
    "    initcap,\n",
    "    regexp_extract,\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    coalesce,\n",
    "    to_timestamp,\n",
    "    count,\n",
    "    sum,\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Text Standardisation\n",
    "df_clean = (\n",
    "    df_ecommerce.withColumn(\n",
    "        \"product_clean\",\n",
    "        initcap(trim(regexp_replace(col(\"product\"), r\"[^a-zA-z0-9\\s]\", \"\"))),\n",
    "    )\n",
    "    .withColumn(\"category_clean\", initcap(trim(col(\"category\"))))\n",
    "    .withColumn(\"status_clean\", initcap(trim(col(\"status\"))))\n",
    ")\n",
    "\n",
    "# Email Validation\n",
    "email_pattern = r\"^[\\w.+-]+@[\\w-]+(\\.[\\w-]+)+$\"\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"email_clean\",\n",
    "    when(\n",
    "        lower(trim(col(\"email\"))).rlike(email_pattern), lower(trim(col(\"email\")))\n",
    "    ).otherwise(None),\n",
    ")\n",
    "\n",
    "# Price Cleaning\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"price_clean\", regexp_replace(col(\"price\"), r\"[$,]\", \"\").cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Quantity Cleaning\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"quantity_clean\",\n",
    "    when(col(\"quantity\").rlike(\"^\\d+$\"), col(\"quantity\").cast(IntegerType())).otherwise(\n",
    "        None\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Date Parsing\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"order_date_clean\",\n",
    "    coalesce(\n",
    "        to_timestamp(col(\"order_date\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        to_timestamp(col(\"order_date\"), \"dd/MM/yyyy\"),\n",
    "        to_timestamp(col(\"order_date\"), \"MMMM dd, yyyy\"),\n",
    "        to_timestamp(col(\"order_date\"), \"dd-MM-yyyy\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Data Quality Report\n",
    "print(\"Data Quality Report\")\n",
    "\n",
    "df_quality = df_clean.select(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    sum(col(\"product_clean\").isNull().cast(\"int\")).alias(\"missing_product\"),\n",
    "    sum(col(\"email_clean\").isNull().cast(\"int\")).alias(\"invalid_email\"),\n",
    "    sum(col(\"price_clean\").isNull().cast(\"int\")).alias(\"invalid_price\"),\n",
    "    sum(col(\"quantity_clean\").isNull().cast(\"int\")).alias(\"invalid_quantity\"),\n",
    "    sum(col(\"order_date_clean\").isNull().cast(\"int\")).alias(\"invalid_date\"),\n",
    ")\n",
    "\n",
    "df_quality.show()\n",
    "\n",
    "\n",
    "# Extract Useful Regex Info (Product Size)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"size_extracted\", regexp_extract(col(\"product\"), r\"Size (\\d+)\", 1)\n",
    ")\n",
    "\n",
    "\n",
    "# Clean Output\n",
    "df_final = df_clean.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"product_clean\").alias(\"product\"),\n",
    "    col(\"category_clean\").alias(\"category\"),\n",
    "    col(\"email_clean\").alias(\"email\"),\n",
    "    col(\"price_clean\").alias(\"price\"),\n",
    "    col(\"quantity_clean\").alias(\"quantity\"),\n",
    "    col(\"order_date_clean\").alias(\"order_date\"),\n",
    "    col(\"status_clean\").alias(\"status\"),\n",
    "    col(\"size_extracted\"),\n",
    ")\n",
    "\n",
    "print(\"Final Cleaned Dataset:\")\n",
    "df_final.show(truncate=False)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b86d0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
